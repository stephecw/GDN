{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d5f991",
   "metadata": {},
   "source": [
    "# GDN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0250eed",
   "metadata": {},
   "source": [
    "## Model code from the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f464feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter, Linear, Sequential, BatchNorm1d, ReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "import time\n",
    "import math\n",
    "\n",
    "class GraphLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True,\n",
    "                 negative_slope=0.2, dropout=0, bias=True, inter_dim=-1,**kwargs):\n",
    "        super(GraphLayer, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.__alpha__ = None\n",
    "\n",
    "        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        glorot(self.att_i)\n",
    "        glorot(self.att_j)\n",
    "        \n",
    "        zeros(self.att_em_i)\n",
    "        zeros(self.att_em_j)\n",
    "\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, embedding, return_attention_weights=False):\n",
    "        \"\"\"\"\"\"\n",
    "        if torch.is_tensor(x):\n",
    "            x = self.lin(x)\n",
    "            x = (x, x)\n",
    "        else:\n",
    "            x = (self.lin(x[0]), self.lin(x[1]))\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index,\n",
    "                                       num_nodes=x[1].size(self.node_dim))\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, embedding=embedding, edges=edge_index,\n",
    "                             return_attention_weights=return_attention_weights)\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if return_attention_weights:\n",
    "            alpha, self.__alpha__ = self.__alpha__, None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index_i, size_i,\n",
    "                embedding,\n",
    "                edges,\n",
    "                return_attention_weights):\n",
    "\n",
    "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding_i, embedding_j = embedding[edge_index_i], embedding[edges[0]]\n",
    "            embedding_i = embedding_i.unsqueeze(1).repeat(1,self.heads,1)\n",
    "            embedding_j = embedding_j.unsqueeze(1).repeat(1,self.heads,1)\n",
    "\n",
    "            key_i = torch.cat((x_i, embedding_i), dim=-1)\n",
    "            key_j = torch.cat((x_j, embedding_j), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "        cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)\n",
    "        cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n",
    "\n",
    "        alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(-1)\n",
    "\n",
    "\n",
    "        alpha = alpha.view(-1, self.heads, 1)\n",
    "\n",
    "\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index_i, size_i)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            self.__alpha__ = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c7c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from util.time import *\n",
    "from util.env import *\n",
    "from torch_geometric.nn import GCNConv, GATConv, EdgeConv\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_batch_edge_index(org_edge_index, batch_num, node_num):\n",
    "    # org_edge_index:(2, edge_num)\n",
    "    edge_index = org_edge_index.clone().detach()\n",
    "    edge_num = org_edge_index.shape[1]\n",
    "    batch_edge_index = edge_index.repeat(1,batch_num).contiguous()\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        batch_edge_index[:, i*edge_num:(i+1)*edge_num] += i*node_num\n",
    "\n",
    "    return batch_edge_index.long()\n",
    "\n",
    "\n",
    "class OutLayer(nn.Module):\n",
    "    def __init__(self, in_num, node_num, layer_num, inter_num = 512):\n",
    "        super(OutLayer, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        for i in range(layer_num):\n",
    "            # last layer, output shape:1\n",
    "            if i == layer_num-1:\n",
    "                modules.append(nn.Linear( in_num if layer_num == 1 else inter_num, 1))\n",
    "            else:\n",
    "                layer_in_num = in_num if i == 0 else inter_num\n",
    "                modules.append(nn.Linear( layer_in_num, inter_num ))\n",
    "                modules.append(nn.BatchNorm1d(inter_num))\n",
    "                modules.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        for mod in self.mlp:\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                out = out.permute(0,2,1)\n",
    "                out = mod(out)\n",
    "                out = out.permute(0,2,1)\n",
    "            else:\n",
    "                out = mod(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, inter_dim=0, heads=1, node_num=100):\n",
    "        super(GNNLayer, self).__init__()\n",
    "\n",
    "\n",
    "        self.gnn = GraphLayer(in_channel, out_channel, inter_dim=inter_dim, heads=heads, concat=False)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, embedding=None, node_num=0):\n",
    "\n",
    "        out, (new_edge_index, att_weight) = self.gnn(x, edge_index, embedding, return_attention_weights=True)\n",
    "        self.att_weight_1 = att_weight\n",
    "        self.edge_index_1 = new_edge_index\n",
    "  \n",
    "        out = self.bn(out)\n",
    "        \n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class GDN(nn.Module):\n",
    "    def __init__(self, edge_index_sets, node_num, dim=64, out_layer_inter_dim=256, input_dim=10, out_layer_num=1, topk=20):\n",
    "\n",
    "        super(GDN, self).__init__()\n",
    "\n",
    "        self.edge_index_sets = edge_index_sets\n",
    "\n",
    "        device = get_device()\n",
    "\n",
    "        edge_index = edge_index_sets[0]\n",
    "\n",
    "\n",
    "        embed_dim = dim\n",
    "        self.embedding = nn.Embedding(node_num, embed_dim)\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "\n",
    "        edge_set_num = len(edge_index_sets)\n",
    "        self.gnn_layers = nn.ModuleList([\n",
    "            GNNLayer(input_dim, dim, inter_dim=dim+embed_dim, heads=1) for i in range(edge_set_num)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.topk = topk\n",
    "        self.learned_graph = None\n",
    "\n",
    "        self.out_layer = OutLayer(dim*edge_set_num, node_num, out_layer_num, inter_num = out_layer_inter_dim)\n",
    "\n",
    "        self.cache_edge_index_sets = [None] * edge_set_num\n",
    "        self.cache_embed_index = None\n",
    "\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "\n",
    "    def forward(self, data, org_edge_index):\n",
    "\n",
    "        x = data.clone().detach()\n",
    "        edge_index_sets = self.edge_index_sets\n",
    "\n",
    "        device = data.device\n",
    "\n",
    "        batch_num, node_num, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "\n",
    "\n",
    "        gcn_outs = []\n",
    "        for i, edge_index in enumerate(edge_index_sets):\n",
    "            edge_num = edge_index.shape[1]\n",
    "            cache_edge_index = self.cache_edge_index_sets[i]\n",
    "\n",
    "            if cache_edge_index is None or cache_edge_index.shape[1] != edge_num*batch_num:\n",
    "                self.cache_edge_index_sets[i] = get_batch_edge_index(edge_index, batch_num, node_num).to(device)\n",
    "            \n",
    "            batch_edge_index = self.cache_edge_index_sets[i]\n",
    "            \n",
    "            all_embeddings = self.embedding(torch.arange(node_num).to(device))\n",
    "\n",
    "            weights_arr = all_embeddings.detach().clone()\n",
    "            all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "\n",
    "            weights = weights_arr.view(node_num, -1)\n",
    "\n",
    "            cos_ji_mat = torch.matmul(weights, weights.T)\n",
    "            normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n",
    "            cos_ji_mat = cos_ji_mat / normed_mat\n",
    "\n",
    "            dim = weights.shape[-1]\n",
    "            topk_num = self.topk\n",
    "\n",
    "            topk_indices_ji = torch.topk(cos_ji_mat, topk_num, dim=-1)[1]\n",
    "\n",
    "            self.learned_graph = topk_indices_ji\n",
    "\n",
    "            gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n",
    "            gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "            gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "\n",
    "            batch_gated_edge_index = get_batch_edge_index(gated_edge_index, batch_num, node_num).to(device)\n",
    "            gcn_out = self.gnn_layers[i](x, batch_gated_edge_index, node_num=node_num*batch_num, embedding=all_embeddings)\n",
    "\n",
    "            \n",
    "            gcn_outs.append(gcn_out)\n",
    "\n",
    "        x = torch.cat(gcn_outs, dim=1)\n",
    "        x = x.view(batch_num, node_num, -1)\n",
    "\n",
    "\n",
    "        indexes = torch.arange(0,node_num).to(device)\n",
    "        out = torch.mul(x, self.embedding(indexes))\n",
    "        \n",
    "        out = out.permute(0,2,1)\n",
    "        out = F.relu(self.bn_outlayer_in(out))\n",
    "        out = out.permute(0,2,1)\n",
    "\n",
    "        out = self.dp(out)\n",
    "        out = self.out_layer(out)\n",
    "        out = out.view(-1, node_num)\n",
    "   \n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f5bb2",
   "metadata": {},
   "source": [
    "## Graph construction code from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6578be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def get_most_common_features(target, all_features, max = 3, min = 3):\n",
    "    res = []\n",
    "    main_keys = target.split('_')\n",
    "\n",
    "    for feature in all_features:\n",
    "        if target == feature:\n",
    "            continue\n",
    "\n",
    "        f_keys = feature.split('_')\n",
    "        common_key_num = len(list(set(f_keys) & set(main_keys)))\n",
    "\n",
    "        if common_key_num >= min and common_key_num <= max:\n",
    "            res.append(feature)\n",
    "\n",
    "    return res\n",
    "\n",
    "def build_net(target, all_features):\n",
    "    # get edge_indexes, and index_feature_map\n",
    "    main_keys = target.split('_')\n",
    "    edge_indexes = [\n",
    "        [],\n",
    "        []\n",
    "    ]\n",
    "    index_feature_map = [target]\n",
    "\n",
    "    # find closest features(nodes):\n",
    "    parent_list = [target]\n",
    "    graph_map = {}\n",
    "    depth = 2\n",
    "    \n",
    "    for i in range(depth):        \n",
    "        for feature in parent_list:\n",
    "            children = get_most_common_features(feature, all_features)\n",
    "\n",
    "            if feature not in graph_map:\n",
    "                graph_map[feature] = []\n",
    "            \n",
    "            # exclude parent\n",
    "            pure_children = []\n",
    "            for child in children:\n",
    "                if child not in graph_map:\n",
    "                    pure_children.append(child)\n",
    "\n",
    "            graph_map[feature] = pure_children\n",
    "\n",
    "            if feature not in index_feature_map:\n",
    "                index_feature_map.append(feature)\n",
    "            p_index = index_feature_map.index(feature)\n",
    "            for child in pure_children:\n",
    "                if child not in index_feature_map:\n",
    "                    index_feature_map.append(child)\n",
    "                c_index = index_feature_map.index(child)\n",
    "\n",
    "                edge_indexes[1].append(p_index)\n",
    "                edge_indexes[0].append(c_index)\n",
    "\n",
    "        parent_list = pure_children\n",
    "\n",
    "    return edge_indexes, index_feature_map\n",
    "\n",
    "\n",
    "def construct_data(data, feature_map, labels=0):\n",
    "    res = []\n",
    "\n",
    "    for feature in feature_map:\n",
    "        if feature in data.columns:\n",
    "            res.append(data.loc[:, feature].values.tolist())\n",
    "        else:\n",
    "            print(feature, 'not exist in data')\n",
    "    # append labels as last\n",
    "    sample_n = len(res[0])\n",
    "\n",
    "    if type(labels) == int:\n",
    "        res.append([labels]*sample_n)\n",
    "    elif len(labels) == sample_n:\n",
    "        res.append(labels)\n",
    "\n",
    "    return res\n",
    "\n",
    "def build_loc_net(struc, all_features, feature_map=[]):\n",
    "\n",
    "    index_feature_map = feature_map\n",
    "    edge_indexes = [\n",
    "        [],\n",
    "        []\n",
    "    ]\n",
    "    for node_name, node_list in struc.items():\n",
    "        if node_name not in all_features:\n",
    "            continue\n",
    "\n",
    "        if node_name not in index_feature_map:\n",
    "            index_feature_map.append(node_name)\n",
    "        \n",
    "        p_index = index_feature_map.index(node_name)\n",
    "        for child in node_list:\n",
    "            if child not in all_features:\n",
    "                continue\n",
    "\n",
    "            if child not in index_feature_map:\n",
    "                print(f'error: {child} not in index_feature_map')\n",
    "                # index_feature_map.append(child)\n",
    "\n",
    "            c_index = index_feature_map.index(child)\n",
    "            # edge_indexes[0].append(p_index)\n",
    "            # edge_indexes[1].append(c_index)\n",
    "            edge_indexes[0].append(c_index)\n",
    "            edge_indexes[1].append(p_index)\n",
    "        \n",
    "\n",
    "    \n",
    "    return edge_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e6b78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "def get_feature_map(dataset):\n",
    "    feature_file = open(f'./data/{dataset}/list.txt', 'r')\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "\n",
    "    return feature_list\n",
    "# graph is 'fully-connect'\n",
    "def get_fc_graph_struc(dataset):\n",
    "    feature_file = open(f'./data/{dataset}/list.txt', 'r')\n",
    "\n",
    "    struc_map = {}\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "\n",
    "    for ft in feature_list:\n",
    "        if ft not in struc_map:\n",
    "            struc_map[ft] = []\n",
    "\n",
    "        for other_ft in feature_list:\n",
    "            if other_ft is not ft:\n",
    "                struc_map[ft].append(other_ft)\n",
    "    \n",
    "    return struc_map\n",
    "\n",
    "def get_prior_graph_struc(dataset):\n",
    "    feature_file = open(f'./data/{dataset}/features.txt', 'r')\n",
    "\n",
    "    struc_map = {}\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "\n",
    "    for ft in feature_list:\n",
    "        if ft not in struc_map:\n",
    "            struc_map[ft] = []\n",
    "        for other_ft in feature_list:\n",
    "            if dataset == 'wadi' or dataset == 'wadi2':\n",
    "                # same group, 1_xxx, 2A_xxx, 2_xxx\n",
    "                if other_ft is not ft and other_ft[0] == ft[0]:\n",
    "                    struc_map[ft].append(other_ft)\n",
    "            elif dataset == 'swat':\n",
    "                # FIT101, PV101\n",
    "                if other_ft is not ft and other_ft[-3] == ft[-3]:\n",
    "                    struc_map[ft].append(other_ft)\n",
    "\n",
    "    \n",
    "    return struc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f31049",
   "metadata": {},
   "source": [
    "## Train code from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "103b7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from util.time import *\n",
    "from util.env import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from test import *\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from evaluate import get_best_performance_data, get_val_performance_data, get_full_err_scores\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from scipy.stats import iqr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_func(y_pred, y_true):\n",
    "    loss = F.mse_loss(y_pred, y_true, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def train(model = None, save_path = '', config={},  train_dataloader=None, val_dataloader=None, feature_map={}, test_dataloader=None, test_dataset=None, dataset_name='swat', train_dataset=None):\n",
    "\n",
    "    seed = config['seed']\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=config['decay'])\n",
    "\n",
    "    now = time.time()\n",
    "    \n",
    "    train_loss_list = []\n",
    "    cmp_loss_list = []\n",
    "\n",
    "    device = get_device()\n",
    "\n",
    "\n",
    "    acu_loss = 0\n",
    "    min_loss = 1e+8\n",
    "    min_f1 = 0\n",
    "    min_pre = 0\n",
    "    best_prec = 0\n",
    "\n",
    "    i = 0\n",
    "    epoch = config['epoch']\n",
    "    early_stop_win = 15\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    log_interval = 1000\n",
    "    stop_improve_count = 0\n",
    "\n",
    "    dataloader = train_dataloader\n",
    "\n",
    "    for i_epoch in range(epoch):\n",
    "\n",
    "        acu_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for x, labels, attack_labels, edge_index in dataloader:\n",
    "            _start = time.time()\n",
    "\n",
    "            x, labels, edge_index = [item.float().to(device) for item in [x, labels, edge_index]]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x, edge_index).float().to(device)\n",
    "            loss = loss_func(out, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            train_loss_list.append(loss.item())\n",
    "            acu_loss += loss.item()\n",
    "                \n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # each epoch\n",
    "        print('epoch ({} / {}) (Loss:{:.8f}, ACU_loss:{:.8f})'.format(\n",
    "                        i_epoch, epoch, \n",
    "                        acu_loss/len(dataloader), acu_loss), flush=True\n",
    "            )\n",
    "\n",
    "        # use val dataset to judge\n",
    "        if val_dataloader is not None:\n",
    "\n",
    "            val_loss, val_result = test(model, val_dataloader)\n",
    "\n",
    "            if val_loss < min_loss:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "\n",
    "                min_loss = val_loss\n",
    "                stop_improve_count = 0\n",
    "            else:\n",
    "                stop_improve_count += 1\n",
    "\n",
    "\n",
    "            if stop_improve_count >= early_stop_win:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            if acu_loss < min_loss :\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                min_loss = acu_loss\n",
    "\n",
    "\n",
    "\n",
    "    return train_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05d11d",
   "metadata": {},
   "source": [
    "## Test code from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce7c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from util.time import *\n",
    "from util.env import *\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from util.data import *\n",
    "from util.preprocess import *\n",
    "\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    # test\n",
    "    loss_func = nn.MSELoss(reduction='mean')\n",
    "    device = get_device()\n",
    "\n",
    "    test_loss_list = []\n",
    "    now = time.time()\n",
    "\n",
    "    test_predicted_list = []\n",
    "    test_ground_list = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    t_test_predicted_list = []\n",
    "    t_test_ground_list = []\n",
    "    t_test_labels_list = []\n",
    "\n",
    "    test_len = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    acu_loss = 0\n",
    "    for x, y, labels, edge_index in dataloader:\n",
    "        x, y, labels, edge_index = [item.to(device).float() for item in [x, y, labels, edge_index]]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted = model(x, edge_index).float().to(device)\n",
    "            \n",
    "            \n",
    "            loss = loss_func(predicted, y)\n",
    "            \n",
    "\n",
    "            labels = labels.unsqueeze(1).repeat(1, predicted.shape[1])\n",
    "\n",
    "            if len(t_test_predicted_list) <= 0:\n",
    "                t_test_predicted_list = predicted\n",
    "                t_test_ground_list = y\n",
    "                t_test_labels_list = labels\n",
    "            else:\n",
    "                t_test_predicted_list = torch.cat((t_test_predicted_list, predicted), dim=0)\n",
    "                t_test_ground_list = torch.cat((t_test_ground_list, y), dim=0)\n",
    "                t_test_labels_list = torch.cat((t_test_labels_list, labels), dim=0)\n",
    "        \n",
    "        test_loss_list.append(loss.item())\n",
    "        acu_loss += loss.item()\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "        if i % 10000 == 1 and i > 1:\n",
    "            print(timeSincePlus(now, i / test_len))\n",
    "\n",
    "\n",
    "    test_predicted_list = t_test_predicted_list.tolist()        \n",
    "    test_ground_list = t_test_ground_list.tolist()        \n",
    "    test_labels_list = t_test_labels_list.tolist()      \n",
    "    \n",
    "    avg_loss = sum(test_loss_list)/len(test_loss_list)\n",
    "\n",
    "    return avg_loss, [test_predicted_list, test_ground_list, test_labels_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
