{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d5f991",
   "metadata": {},
   "source": [
    "# GDN - Multivariate time series anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ade0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, time, math, random\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import Parameter, Linear, BatchNorm1d, ReLU\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
    "from scipy.stats import iqr\n",
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.ar_model import AutoReg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b887ca3",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7d7833",
   "metadata": {},
   "source": [
    "### Model (From original paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_edge_index(org_edge_index, batch_num, node_num):\n",
    "    \"\"\" Convert single graph edge_index to batched graph edge_index\n",
    "    Args:\n",
    "        org_edge_index: torch.Tensor, shape (2, E)\n",
    "        batch_num: int, number of graphs in the batch\n",
    "        node_num: int, number of nodes in a single graph\n",
    "    Returns:\n",
    "        batch_edge_index: torch.Tensor, shape (2, E*batch_num)\"\"\"\n",
    "    edge_index = org_edge_index.clone().detach()\n",
    "    edge_num = org_edge_index.shape[1]\n",
    "    batch_edge_index = edge_index.repeat(1,batch_num).contiguous()\n",
    "    for i in range(batch_num):\n",
    "        batch_edge_index[:, i*edge_num:(i+1)*edge_num] += i*node_num\n",
    "    return batch_edge_index.long()\n",
    "\n",
    "class GraphLayer(MessagePassing):\n",
    "    \"\"\" Graph Attention Layer with Edge Embeddings \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True,\n",
    "                 negative_slope=0.2, dropout=0, bias=True, inter_dim=-1,**kwargs):\n",
    "        \"\"\" Args:\n",
    "            in_channels: int, input feature dimension\n",
    "            out_channels: int, output feature dimension\n",
    "            heads: int, number of attention heads\n",
    "            concat: bool, whether to concatenate multi-head results\n",
    "            negative_slope: float, LeakyReLU negative slope\n",
    "            dropout: float, dropout rate on attention weights\n",
    "            bias: bool, whether to add bias term \"\"\"\n",
    "        super(GraphLayer, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.node_dim = 0\n",
    "        self.__alpha__ = None\n",
    "        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        glorot(self.att_i)\n",
    "        glorot(self.att_j)\n",
    "        zeros(self.att_em_i)\n",
    "        zeros(self.att_em_j)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, embedding, return_attention_weights=False):\n",
    "        if torch.is_tensor(x):\n",
    "            x = self.lin(x)\n",
    "            x = (x, x)\n",
    "        else:\n",
    "            x = (self.lin(x[0]), self.lin(x[1]))\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index,\n",
    "                                       num_nodes=x[1].size(self.node_dim))\n",
    "        out = self.propagate(edge_index, x=x, embedding=embedding, edges=edge_index,\n",
    "                             return_attention_weights=return_attention_weights)\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        if return_attention_weights:\n",
    "            alpha, self.__alpha__ = self.__alpha__, None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index_i, size_i,\n",
    "                embedding,\n",
    "                edges,\n",
    "                return_attention_weights):\n",
    "\n",
    "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding_i, embedding_j = embedding[edge_index_i], embedding[edges[0]]\n",
    "            embedding_i = embedding_i.unsqueeze(1).repeat(1,self.heads,1)\n",
    "            embedding_j = embedding_j.unsqueeze(1).repeat(1,self.heads,1)\n",
    "            key_i = torch.cat((x_i, embedding_i), dim=-1)\n",
    "            key_j = torch.cat((x_j, embedding_j), dim=-1)\n",
    "        cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)\n",
    "        cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n",
    "\n",
    "        alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(-1) # alpha: (E, heads)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = alpha.mean(dim=1)          # (E,)\n",
    "        alpha = softmax(alpha, edge_index_i)   # (E,)\n",
    "        alpha = alpha.view(-1, 1, 1)       # (E, 1, 1)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            self.__alpha__ = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n",
    "\n",
    "class OutLayer(nn.Module):\n",
    "    \"\"\" Output MLP Layer \"\"\"\n",
    "    def __init__(self, in_num, node_num, layer_num, inter_num = 512):\n",
    "        super(OutLayer, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        for i in range(layer_num):\n",
    "            # last layer, output shape:1\n",
    "            if i == layer_num-1:\n",
    "                modules.append(nn.Linear( in_num if layer_num == 1 else inter_num, 1))\n",
    "            else:\n",
    "                layer_in_num = in_num if i == 0 else inter_num\n",
    "                modules.append(nn.Linear( layer_in_num, inter_num ))\n",
    "                modules.append(nn.BatchNorm1d(inter_num))\n",
    "                modules.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        for mod in self.mlp:\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                out = out.permute(0,2,1)\n",
    "                out = mod(out)\n",
    "                out = out.permute(0,2,1)\n",
    "            else:\n",
    "                out = mod(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, inter_dim=0, heads=1, node_num=100):\n",
    "        super(GNNLayer, self).__init__()\n",
    "\n",
    "\n",
    "        self.gnn = GraphLayer(in_channel, out_channel, inter_dim=inter_dim, heads=heads, concat=False)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, embedding=None, node_num=0):\n",
    "\n",
    "        out, (new_edge_index, att_weight) = self.gnn(x, edge_index, embedding, return_attention_weights=True)\n",
    "        self.att_weight_1 = att_weight\n",
    "        self.edge_index_1 = new_edge_index\n",
    "  \n",
    "        out = self.bn(out)\n",
    "        \n",
    "        return self.relu(out)\n",
    "\n",
    "# ------- Main GDN model -------\n",
    "\n",
    "class GDN(nn.Module):\n",
    "    def __init__(self, edge_index_sets, node_num, dim=64, out_layer_inter_dim=256, input_dim=10, out_layer_num=1, topk=20):\n",
    "\n",
    "        super(GDN, self).__init__()\n",
    "\n",
    "        self.edge_index_sets = edge_index_sets\n",
    "\n",
    "        embed_dim = dim\n",
    "        self.embedding = nn.Embedding(node_num, embed_dim)\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "\n",
    "        edge_set_num = len(edge_index_sets)\n",
    "        self.gnn_layers = nn.ModuleList([\n",
    "            GNNLayer(input_dim, dim, inter_dim=dim+embed_dim, heads=1) for i in range(edge_set_num)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.topk = topk\n",
    "        self.learned_graph = None\n",
    "\n",
    "        self.out_layer = OutLayer(dim*edge_set_num, node_num, out_layer_num, inter_num = out_layer_inter_dim)\n",
    "\n",
    "        self.cache_edge_index_sets = [None] * edge_set_num\n",
    "        self.cache_embed_index = None\n",
    "\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = data.clone().detach()\n",
    "        edge_index_sets = self.edge_index_sets\n",
    "\n",
    "        device = data.device\n",
    "\n",
    "        batch_num, node_num, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "\n",
    "\n",
    "        gcn_outs = []\n",
    "        for i, edge_index in enumerate(edge_index_sets):\n",
    "            edge_num = edge_index.shape[1]\n",
    "            cache_edge_index = self.cache_edge_index_sets[i]\n",
    "\n",
    "            if cache_edge_index is None or cache_edge_index.shape[1] != edge_num*batch_num:\n",
    "                self.cache_edge_index_sets[i] = get_batch_edge_index(edge_index, batch_num, node_num).to(device)\n",
    "            \n",
    "            all_embeddings = self.embedding(torch.arange(node_num).to(device))\n",
    "\n",
    "            weights_arr = all_embeddings.detach().clone()\n",
    "            all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "\n",
    "            weights = weights_arr.view(node_num, -1)\n",
    "\n",
    "            cos_ji_mat = torch.matmul(weights, weights.T)\n",
    "            normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n",
    "            cos_ji_mat = cos_ji_mat / normed_mat\n",
    "\n",
    "            topk_num = self.topk\n",
    "\n",
    "            topk_indices_ji = torch.topk(cos_ji_mat, topk_num, dim=-1)[1]\n",
    "\n",
    "            self.learned_graph = topk_indices_ji\n",
    "\n",
    "            gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n",
    "            gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "            gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "\n",
    "            batch_gated_edge_index = get_batch_edge_index(gated_edge_index, batch_num, node_num).to(device)\n",
    "            gcn_out = self.gnn_layers[i](x, batch_gated_edge_index, node_num=node_num*batch_num, embedding=all_embeddings)\n",
    "\n",
    "            \n",
    "            gcn_outs.append(gcn_out)\n",
    "\n",
    "        x = torch.cat(gcn_outs, dim=1)\n",
    "        x = x.view(batch_num, node_num, -1)\n",
    "\n",
    "\n",
    "        indexes = torch.arange(0,node_num).to(device)\n",
    "        out = torch.mul(x, self.embedding(indexes))\n",
    "        \n",
    "        out = out.permute(0,2,1)\n",
    "        out = F.relu(self.bn_outlayer_in(out))\n",
    "        out = out.permute(0,2,1)\n",
    "\n",
    "        out = self.dp(out)\n",
    "        out = self.out_layer(out)\n",
    "        out = out.view(-1, node_num)\n",
    "   \n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7153d74",
   "metadata": {},
   "source": [
    "### Training / testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model=None,\n",
    "    save_path='',\n",
    "    config={},\n",
    "    train_dataloader=None,\n",
    "    val_dataloader=None,\n",
    "    loss_func=nn.MSELoss(reduction='mean')\n",
    "):\n",
    "    \"\"\" Generic training function for:\n",
    "      - train_loader: batches of (x, y)\n",
    "      - val_loader: batches of (x, y)\n",
    "      Args:\n",
    "        model: nn.Module, the model to train\n",
    "        save_path: str, path to save the best model\n",
    "        config: dict, training configuration\n",
    "        train_dataloader: DataLoader, training data loader\n",
    "        val_dataloader: DataLoader, validation data loader (optional)\n",
    "        loss_func: loss function to use during training\n",
    "      Returns:\n",
    "        train_loss_list: list of training losses per batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    seed = config['seed']\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Initialize training components\n",
    "    device = get_device()\n",
    "    loss_func = loss_func\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=config.get('decay', 0), betas=config.get('beta', (0.9, 0.999)))\n",
    "    train_loss_list = []\n",
    "    acu_loss = 0\n",
    "    min_loss = 1e+8\n",
    "    epoch = config['epoch']\n",
    "    early_stop_win = config['early_stop_win']\n",
    "    stop_improve_count = 0\n",
    "    model.train()\n",
    "    dataloader = train_dataloader\n",
    "\n",
    "    for i_epoch in range(epoch):\n",
    "        acu_loss = 0.0\n",
    "        model.train()\n",
    "        for x, y in dataloader:\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x).float().to(device)\n",
    "            loss = loss_func(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_list.append(loss.item())\n",
    "            acu_loss += loss.item()\n",
    "\n",
    "        print(f\"epoch ({i_epoch + 1} / {epoch}) (Loss:{acu_loss / len(dataloader):.8f}, ACU_loss:{acu_loss:.8f})\", flush=True)\n",
    "\n",
    "        # early stopping on val loss\n",
    "        if val_dataloader is not None:\n",
    "            val_loss, _ = test(model, val_dataloader)\n",
    "            if val_loss < min_loss:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                min_loss = val_loss\n",
    "                stop_improve_count = 0\n",
    "            else:\n",
    "                stop_improve_count += 1\n",
    "            if stop_improve_count >= early_stop_win:\n",
    "                break\n",
    "        else:\n",
    "            if acu_loss < min_loss:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                min_loss = acu_loss\n",
    "    return train_loss_list\n",
    "\n",
    "def test(model, dataloader, loss_func=nn.MSELoss(reduction='mean')):\n",
    "    \"\"\"\n",
    "    Generic test function for:\n",
    "      - val_loader: batches of (x, y)\n",
    "      - test_loader: batches of (x, y, labels)\n",
    "    Args:\n",
    "      model: nn.Module, the model to evaluate\n",
    "      dataloader: DataLoader, data loader for evaluation\n",
    "      loss_func: loss function to use during evaluation\n",
    "\n",
    "    Returns:\n",
    "      avg_loss,\n",
    "      [pred_list, gt_list]                 for (x, y) loaders\n",
    "      [pred_list, gt_list, label_list]     for (x, y, labels) loaders\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_func = loss_func\n",
    "    device = get_device()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_pred = []\n",
    "    all_gt = []\n",
    "    all_labels = None  # created only if labels are present\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # manage the two cases: (x, y) or (x, y, labels)\n",
    "        if len(batch) == 2:\n",
    "            x, y = batch\n",
    "            labels = None\n",
    "        elif len(batch) == 3:\n",
    "            x, y, labels = batch\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected batch size {len(batch)} (expected 2 or 3).\")\n",
    "\n",
    "        x = x.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(x).float().to(device)\n",
    "            loss = loss_func(pred, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "        all_pred.append(pred.cpu())\n",
    "        all_gt.append(y.cpu())\n",
    "        if labels is not None:\n",
    "            if all_labels is None:\n",
    "                all_labels = labels.cpu()\n",
    "            else:\n",
    "                all_labels = torch.cat((all_labels, labels.cpu()), dim=0)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_pred = torch.cat(all_pred, dim=0).tolist()\n",
    "    all_gt   = torch.cat(all_gt,   dim=0).tolist()\n",
    "\n",
    "    avg_loss = total_loss / n_batches\n",
    "\n",
    "    if all_labels is not None:\n",
    "        return avg_loss, [all_pred, all_gt, all_labels.tolist()]\n",
    "    else:\n",
    "        return avg_loss, [all_pred, all_gt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026063d",
   "metadata": {},
   "source": [
    "### Anomaly detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mu_iqr(errors, eps=1e-2):\n",
    "    \"\"\"\n",
    "    Compute per-sensor normalization statistics from the given errors.\n",
    "    Used for validation (for threshold) AND for test (for normalized anomaly scores).\n",
    "    \"\"\"\n",
    "    mu = np.median(errors, axis=0)\n",
    "    q75 = np.percentile(errors, 75, axis=0)\n",
    "    q25 = np.percentile(errors, 25, axis=0)\n",
    "    iqr = q75 - q25\n",
    "    iqr = iqr + eps\n",
    "    return mu, iqr\n",
    "\n",
    "\n",
    "def compute_normalized_scores(pred, gt, smooth_window=3):\n",
    "    \"\"\"\n",
    "    Normalization as done in the official GDN implementation.\n",
    "    \"\"\"\n",
    "    # Compute test errors\n",
    "    err = np.abs(pred - gt)            # (T, D)\n",
    "\n",
    "    # Compute mu_test and iqr_test (IMPORTANT: from TEST errors)\n",
    "    mu_test, iqr_test = compute_mu_iqr(err)\n",
    "\n",
    "    # Normalized anomaly per sensor\n",
    "    a = (err - mu_test) / iqr_test            # (T, D)\n",
    "\n",
    "    # Max across sensors = global anomaly score\n",
    "    scores = np.max(a, axis=1)\n",
    "\n",
    "    # Moving-average smoothing window\n",
    "    if smooth_window > 1:\n",
    "        kernel = np.ones(smooth_window) / smooth_window\n",
    "        scores = np.convolve(scores, kernel, mode='same')\n",
    "\n",
    "    return scores, err, a\n",
    "\n",
    "def predict_anomaly_labels(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "def evaluate_anomaly_detection(scores, labels, threshold):\n",
    "    \"\"\" Evaluate point-wise anomaly detection performance.\n",
    "    Args:\n",
    "        scores: list or np.array of float, shape (T,), anomaly scores\n",
    "        labels: list or np.array of int, shape (T,), point-wise anomaly labels (0/1)\n",
    "        threshold: float, anomaly score threshold for point-wise anomaly prediction\n",
    "    Returns:\n",
    "        metrics: dict, containing TP, FP, FN, TN, precision, recall, f1, auc, threshold\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels).astype(int)\n",
    "    y_pred = predict_anomaly_labels(scores, threshold)\n",
    "\n",
    "    return {\n",
    "        \"TP\": sum(1 for i in range(len(labels)) if labels[i] == 1 and y_pred[i] == 1),\n",
    "        \"FP\": sum(1 for i in range(len(labels)) if labels[i] == 0 and y_pred[i] == 1),\n",
    "        \"FN\": sum(1 for i in range(len(labels)) if labels[i] == 1 and y_pred[i] == 0),\n",
    "        \"TN\": sum(1 for i in range(len(labels)) if labels[i] == 0 and y_pred[i] == 0),\n",
    "        \"precision\": precision_score(labels, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(labels, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(labels, y_pred, zero_division=0),\n",
    "        \"auc\": roc_auc_score(labels, scores),\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "def anomaly_sequences(labels):\n",
    "    \"\"\" Extract contiguous anomaly sequences from point-wise labels.\n",
    "    Args:\n",
    "        labels: list or np.array of int, shape (T,), point-wise anomaly labels (0/1)\n",
    "    Returns:\n",
    "        anomalies_seq: list of [start_idx, end_idx] for each contiguous anomaly sequence\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels).astype(int)\n",
    "\n",
    "    anomalies_seq = []\n",
    "    in_seq = False\n",
    "    boundaries = [-1, -1]\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 1:\n",
    "            if not in_seq:\n",
    "                in_seq = True\n",
    "                boundaries[0] = i\n",
    "        else:\n",
    "            if in_seq:\n",
    "                in_seq = False\n",
    "                boundaries[1] = i - 1\n",
    "                anomalies_seq.append(boundaries.copy())\n",
    "    \n",
    "    return anomalies_seq\n",
    "\n",
    "def evaluate_anomaly_detection_on_sequences(scores, labels, threshold):\n",
    "    \"\"\" Evaluate sequence-wise anomaly detection performance. Fuses point-wise predictions within each anomaly sequence.\n",
    "    Args:\n",
    "        scores: list or np.array of float, shape (T,), anomaly scores\n",
    "        labels: list or np.array of int, shape (T,), point-wise anomaly labels (0/1)\n",
    "        threshold: float, anomaly score threshold for point-wise anomaly prediction\n",
    "    Returns:\n",
    "        metrics: dict, containing TP, FP, FN, TN, precision, recall, f1, auc, threshold\n",
    "    \"\"\"\n",
    "    labels = np.asarray(labels).astype(int)\n",
    "    y_pred = predict_anomaly_labels(scores, threshold)\n",
    "\n",
    "    anomalies_seq = anomaly_sequences(labels)\n",
    "    labels_agg_seq = []\n",
    "    y_pred_agg_seq = []\n",
    "    scores_agg_seq = []\n",
    "    for i in range(len(labels)):\n",
    "      if labels[i] == 0:\n",
    "            labels_agg_seq.append(labels[i])\n",
    "            y_pred_agg_seq.append(y_pred[i])\n",
    "            scores_agg_seq.append(scores[i])\n",
    "    for (start, end) in anomalies_seq:\n",
    "        labels_agg_seq.append(1)\n",
    "        if np.sum(y_pred[start:end+1]) > 0:\n",
    "            y_pred_agg_seq.append(1)\n",
    "        else:\n",
    "            y_pred_agg_seq.append(0)\n",
    "        scores_agg_seq.append(np.max(scores[start:end+1]))\n",
    "        \n",
    "    return {\n",
    "        \"TP\": sum(1 for i in range(len(labels_agg_seq)) if labels_agg_seq[i] == 1 and y_pred_agg_seq[i] == 1),\n",
    "        \"FP\": sum(1 for i in range(len(labels_agg_seq)) if labels_agg_seq[i] == 0 and y_pred_agg_seq[i] == 1),\n",
    "        \"FN\": sum(1 for i in range(len(labels_agg_seq)) if labels_agg_seq[i] == 1 and y_pred_agg_seq[i] == 0),\n",
    "        \"TN\": sum(1 for i in range(len(labels_agg_seq)) if labels_agg_seq[i] == 0 and y_pred_agg_seq[i] == 0),\n",
    "        \"precision\": precision_score(labels_agg_seq, y_pred_agg_seq, zero_division=0),\n",
    "        \"recall\": recall_score(labels_agg_seq, y_pred_agg_seq, zero_division=0),\n",
    "        \"f1\": f1_score(labels_agg_seq, y_pred_agg_seq, zero_division=0),\n",
    "        \"auc\": roc_auc_score(labels_agg_seq, scores_agg_seq),\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e5b94",
   "metadata": {},
   "source": [
    "### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a74cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(data, window_size):\n",
    "    \"\"\"\n",
    "    Creates overlapping sequences (sliding windows) from the time series data.\n",
    "    Input: (T, D) -> Output: (N_samples, D, Window_Size)\n",
    "    \"\"\"\n",
    "    N_time, _ = data.shape\n",
    "    windows = []\n",
    "    # Loop from 0 up to the point where a full window can be created\n",
    "    for i in range(N_time - window_size):\n",
    "        windows.append(data[i : i + window_size, :].T) # Transpose to (D, Window_Size) \n",
    "    return np.array(windows)\n",
    "\n",
    "def data_analyze(x, sensor_id=0, window_var=200):\n",
    "    T = len(x)\n",
    "    \n",
    "    # Signal plot\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(x)\n",
    "    plt.title(f\"Sensor {sensor_id} - Signal (raw)\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.show()\n",
    "\n",
    "    # Local variance sliding window\n",
    "    var_local = []\n",
    "    for i in range(T - window_var):\n",
    "        var_local.append(np.var(x[i:i+window_var]))\n",
    "    var_local = np.array(var_local)\n",
    "    \n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(var_local)\n",
    "    plt.title(f\"Sensor {sensor_id} - Local variance (window={window_var})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Autocorrelation function\n",
    "    lag_acf = acf(x, nlags=200, fft=True)\n",
    "    \n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(range(len(lag_acf)), lag_acf)\n",
    "    plt.title(f\"Sensor {sensor_id} - Autocorrelation function\")\n",
    "    plt.xlabel(\"Lag\")\n",
    "    plt.show()\n",
    "    \n",
    "    # DFT Spectrum\n",
    "    freqs = np.fft.rfftfreq(T)\n",
    "    amp = np.abs(np.fft.rfft(x))\n",
    "\n",
    "    plt.figure(figsize=(12,3))\n",
    "    plt.plot(freqs, amp)\n",
    "    plt.title(f\"Sensor {sensor_id} - DFT Spectrum\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "def check_nan_inf(data, name):\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Warning: {name} contains NaN values.\")\n",
    "    if np.isinf(data).any():\n",
    "        print(f\"Warning: {name} contains Inf values.\")\n",
    "    if not np.isnan(data).any() and not np.isinf(data).any():\n",
    "        print(f\"{name} contains no NaN or Inf values.\")\n",
    "\n",
    "def basic_statistics(data):\n",
    "    print(f\"Length : {data.shape[0]}\")\n",
    "    print(f\"Num of sensors (features): {data.shape[1]}\")\n",
    "    unique_val = {}\n",
    "    repartition = [0,0]\n",
    "    nb_continous = 0\n",
    "    nb_discrete = 0\n",
    "    for i in range(data.shape[1]):\n",
    "        unique_vals, count = np.unique(data[:, i], return_counts=True)\n",
    "        n_unique = len(unique_vals)\n",
    "        if n_unique <= 2:\n",
    "            for i in range(n_unique):\n",
    "                if unique_vals[i] == 0:\n",
    "                    repartition[0] += count[i]\n",
    "                else:\n",
    "                    repartition[1] += count[i]\n",
    "            nb_discrete += 1\n",
    "            str_unique = ', '.join(map(str, unique_vals))\n",
    "            in_unique = False\n",
    "            for k in unique_val.keys():\n",
    "                if k == str_unique:\n",
    "                    unique_val[k] += 1\n",
    "                    in_unique = True\n",
    "                    break\n",
    "            if not in_unique:\n",
    "                unique_val[str_unique] = 1 \n",
    "        else:\n",
    "            nb_continous += 1\n",
    "    print(f\"Number of continuous features: {nb_continous}\")\n",
    "    print(f\"Number of discrete features: {nb_discrete}\")\n",
    "    print(\"Discrete feature value distributions:\")\n",
    "    for k, v in unique_val.items():\n",
    "        print(f\"    Values: [{k}] -> Count: {v} feature(s)\")\n",
    "    print(f\"    Repartition of discrete feature values: \"\n",
    "          f\"value 0 : {repartition[0]} ({100*repartition[0]/(repartition[0]+repartition[1]):.2f} %), \" \n",
    "          f\"value 1 : {repartition[1]} ({100*repartition[1]/(repartition[0]+repartition[1]):.2f} %)\")\n",
    "\n",
    "def stat_labels(labels):\n",
    "    n_anomalies = np.sum(labels)\n",
    "    n_normal = labels.shape[0] - n_anomalies\n",
    "    print(f\"Total number of points: {labels.shape[0]}\")\n",
    "    print(f\"Number of normal points: {n_normal} ({100*n_normal/labels.shape[0]:.2f} %)\")\n",
    "    print(f\"Number of anomalous points: {n_anomalies} ({100*n_anomalies/labels.shape[0]:.2f} %)\")\n",
    "    ongoing = 0\n",
    "    nb_anomaly_sequences = 0\n",
    "    nb_isolated_anomalies = 0\n",
    "    length_anomaly_sequences = []\n",
    "    for l in labels:\n",
    "        if l == 1:\n",
    "            ongoing += 1\n",
    "        if l == 0:\n",
    "            if ongoing > 1:\n",
    "                nb_anomaly_sequences += 1\n",
    "                length_anomaly_sequences.append(ongoing)\n",
    "                ongoing = 0\n",
    "            elif ongoing == 1:\n",
    "                nb_isolated_anomalies += 1\n",
    "                ongoing = 0\n",
    "    if ongoing > 1:\n",
    "        nb_anomaly_sequences += 1\n",
    "    elif ongoing == 1:\n",
    "        nb_isolated_anomalies += 1\n",
    "    length_anomaly_sequences = np.array(length_anomaly_sequences)\n",
    "    print(f\"Number of isolated anomalies (length = 1): {nb_isolated_anomalies}\")\n",
    "    print(f\"Number of anomaly sequences (length > 1): {nb_anomaly_sequences}\")\n",
    "    if length_anomaly_sequences.size > 0:\n",
    "        print(f\"    Anomaly sequence lengths -- Min: {np.min(length_anomaly_sequences)}, \"\n",
    "              f\"    Max: {np.max(length_anomaly_sequences)}, \"\n",
    "              f\"    Mean: {np.mean(length_anomaly_sequences):.2f}, \"\n",
    "              f\"    Median: {np.median(length_anomaly_sequences)}\")\n",
    "        \n",
    "def detrend_least_squares(x, order=1):\n",
    "    \"\"\"\n",
    "    Remove polynomial trend\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    T = len(x)\n",
    "    t = np.arange(T)\n",
    "\n",
    "    # Construct design matrix B = [t^0, t^1, ..., t^order]\n",
    "    B = np.vstack([t**k for k in range(order + 1)]).T\n",
    "\n",
    "    # Least-squares fitting to find polynomial coefficients\n",
    "    alpha_hat, _, _, _ = np.linalg.lstsq(B, x, rcond=None)\n",
    "\n",
    "    # Trend reconstruction\n",
    "    x_trend = B @ alpha_hat\n",
    "\n",
    "    # Detrended signal\n",
    "    x_detrended = x - x_trend\n",
    "\n",
    "    return x_detrended, x_trend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bc4d8",
   "metadata": {},
   "source": [
    "# SMAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './data' \n",
    "TRAIN_DATA_PATH = BASE_DIR + '/SMAP/SMAP/SMAP_train.npy'\n",
    "TEST_DATA_PATH = BASE_DIR + '/SMAP/SMAP/SMAP_test.npy'\n",
    "TEST_LABEL_PATH = BASE_DIR + '/SMAP/SMAP/SMAP_test_label.npy'\n",
    "    \n",
    "print(\"--- Loading Raw Data ---\")\n",
    "\n",
    "try: ## Try to load the data files\n",
    "    X_train_raw = np.load(TRAIN_DATA_PATH)\n",
    "    X_test_raw = np.load(TEST_DATA_PATH)\n",
    "    y_test_raw = np.load(TEST_LABEL_PATH)\n",
    "except FileNotFoundError as e: # If files not found, download the dataset\n",
    "    print(f\"Error: {e}. Download the dataset and place it in the specified directory.\")\n",
    "    from torch_timeseries.dataset import SMAP\n",
    "    SMAP(root=BASE_DIR)\n",
    "    print(\"Dataset downloaded.\")\n",
    "    X_train_raw = np.load(TRAIN_DATA_PATH)\n",
    "    X_test_raw = np.load(TEST_DATA_PATH)\n",
    "    y_test_raw = np.load(TEST_LABEL_PATH)\n",
    "    \n",
    "print(\"Data Loaded Successfully.\")\n",
    "T_train, D_features = X_train_raw.shape\n",
    "T_test, _ = X_test_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7bef3",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d960aa",
   "metadata": {},
   "source": [
    "### Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbf59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [0,1,2,4] \n",
    "features_to_plot = X_train_raw[:, feature_indices]\n",
    "time_index = np.arange(T_train)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 10), sharex=True)\n",
    "axes = axes.flatten()\n",
    "for i, feature_index in enumerate(feature_indices):\n",
    "    ax = axes[i] \n",
    "    ax.plot(\n",
    "        time_index, \n",
    "        features_to_plot[:, i], \n",
    "        label=f'Feature D{feature_index}', \n",
    "        linewidth=0.5\n",
    "    )\n",
    "    # Set title, labels, and grid\n",
    "    ax.set_title(f'Feature D{feature_index} (Raw Data)', fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "axes[3].set_xlabel('Time Step', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047fd4b5",
   "metadata": {},
   "source": [
    "Plotting the anomaly occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "time_index = np.arange(T_test)\n",
    "# Iterate through the selected features and plot them\n",
    "for i, feature_index in enumerate(feature_indices):\n",
    "    plt.plot(\n",
    "        time_index, \n",
    "        y_test_raw,\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "plt.title(f'Anomalies in Test Data', fontsize=16)\n",
    "plt.xlabel('Time Step', fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"proportion of anomalies: {np.mean(y_test_raw):.4f} in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd11e372",
   "metadata": {},
   "source": [
    "### Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094466b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nan_inf(X_train_raw, \"X_train_raw\")\n",
    "check_nan_inf(X_test_raw, \"X_test_raw\")\n",
    "check_nan_inf(y_test_raw, \"y_test_raw\")\n",
    "\n",
    "print(\"\\n--- Basic Statistics for X_train_raw ---\")\n",
    "basic_statistics(X_train_raw)\n",
    "\n",
    "print(\"\\n--- Basic Statistics for X_test_raw ---\")\n",
    "basic_statistics(X_test_raw)\n",
    "\n",
    "print(\"\\n--- Basic Statistics for y_test_raw ---\")\n",
    "stat_labels(y_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea33a3",
   "metadata": {},
   "source": [
    "### Further analisis for stationarity, noise check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7bb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id = 0 # We choose the sensor we want to analyze\n",
    "data_analyze(X_train_raw[:, sensor_id], sensor_id=sensor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2396e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id = 21\n",
    "data_analyze(X_train_raw[:, sensor_id], sensor_id=sensor_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9822b",
   "metadata": {},
   "source": [
    "### Detrending experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id = 0   # sensor 0 \n",
    "\n",
    "# TRAIN\n",
    "x0_train = X_train_raw[:, sensor_id]\n",
    "x0_train_detrended, trend_train = detrend_least_squares(x0_train, order=2)\n",
    "\n",
    "# TEST\n",
    "x0_test = X_test_raw[:, sensor_id]\n",
    "x0_test_detrended, trend_test = detrend_least_squares(x0_test, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.plot(trend_train, label=\"Trend (fit)\")\n",
    "plt.plot(x0_train_detrended, label=\"Detrended\")\n",
    "plt.plot(x0_train, label=\"Original\")\n",
    "plt.legend()\n",
    "plt.title(\"Sensor 0 detrending (train)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828215be",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e3dd5",
   "metadata": {},
   "source": [
    "### VAR(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Fit a multivariate VAR(100)\n",
    "# -----------------------------\n",
    "\n",
    "val_ratio = 0.2\n",
    "X_train, X_val = X_train_raw[:int((1 - val_ratio) * T_train)], X_train_raw[int((1 - val_ratio) * T_train):]\n",
    "X_test = X_test_raw.copy()\n",
    "labels_test = y_test_raw.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val   shape:\", X_val.shape)\n",
    "print(\"Test  shape:\", X_test.shape)\n",
    "print(\"Label Test shape:\", labels_test.shape)\n",
    "\n",
    "# Fit VAR on training data\n",
    "p = 100  # VAR order\n",
    "print(f\"\\nFitting VAR({p}) on all {X_train.shape[1]} sensors...\")\n",
    "var_model = VAR(X_train)\n",
    "var_results = var_model.fit(maxlags=p, ic=None, trend='c')  # fixed lag order\n",
    "try:\n",
    "    print(var_results.summary())\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"Warning: covariance not positive definite, skipping textual summary.\")\n",
    "# Effective order used (should be p)\n",
    "print(\"VAR lag order used (k_ar):\", var_results.k_ar)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) One-step-ahead rolling forecast on test\n",
    "# -----------------------------------------\n",
    "\n",
    "T_test, _ = X_test.shape\n",
    "k_ar = var_results.k_ar  # should be 100\n",
    "print(f\"k_ar: {k_ar}\")\n",
    "\n",
    "# Prediction on val\n",
    "history_val = list(X_val[:k_ar])  # list of length k_ar of vectors (D_features,)\n",
    "val_pred = []  # will store one-step-ahead predictions for the val set\n",
    "for t in range(X_val.shape[0]-k_ar):\n",
    "    # Take last k_ar observations as a (k_ar, D) array\n",
    "    past = np.array(history_val[-k_ar:])   # shape (k_ar, D_features)\n",
    "    \n",
    "    # Forecast the next step (t+1)\n",
    "    y_hat = var_results.forecast(past, steps=1)[0]  # shape (D_features,)\n",
    "    val_pred.append(y_hat)\n",
    "    \n",
    "    # Update history with the *true* val point at time t\n",
    "    history_val.append(X_val[k_ar+t])\n",
    "val_pred = np.array(val_pred) # shape (T_val-k_ar, D_features)\n",
    "val_true = X_val[k_ar:]  # true values from time k_ar to T_val-1\n",
    "\n",
    "# Prediction on test\n",
    "history = list(X_test[:k_ar])  # list of length k_ar of vectors (D_features,)\n",
    "test_pred = []  # will store one-step-ahead predictions for the test set\n",
    "for t in range(T_test-k_ar):\n",
    "    # Take last k_ar observations as a (k_ar, D) array\n",
    "    past = np.array(history[-k_ar:])   # shape (k_ar, D_features)\n",
    "    \n",
    "    # Forecast the next step (t+1)\n",
    "    y_hat = var_results.forecast(past, steps=1)[0]  # shape (D_features,)\n",
    "    test_pred.append(y_hat)\n",
    "    \n",
    "    # Update history with the *true* test point at time t\n",
    "    history.append(X_test[k_ar+t])\n",
    "\n",
    "test_pred = np.array(test_pred) # shape (T_test-k_ar, D_features)\n",
    "test_true = X_test[k_ar:]  # true values from time k_ar to T_test-1\n",
    "labels_true = labels_test[k_ar:]  # true labels from time k_ar to T_test-1\n",
    "\n",
    "print(\"\\nForecast shapes:\")\n",
    "print(\"val_pred:\", val_pred.shape)\n",
    "print(\"val_true:\", val_true.shape)\n",
    "print(\"test_pred:\", test_pred.shape)\n",
    "print(\"test_true:\", test_true.shape)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3) Compute errors and anomaly scores\n",
    "# --------------------------------------\n",
    "\n",
    "val_scores, val_err, val_a = compute_normalized_scores(val_pred, val_true, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(test_pred, test_true, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639d78d",
   "metadata": {},
   "source": [
    "### AR(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Paramètres\n",
    "# -----------------------------\n",
    "sensor_idx = 0           # on ne travaille que sur le sensor 0\n",
    "p = 100          # ordre du modèle AR, ex. 100\n",
    "\n",
    "val_ratio = 0.2\n",
    "X_train, X_val = X_train_raw[:int((1 - val_ratio) * T_train),sensor_idx], X_train_raw[int((1 - val_ratio) * T_train):,sensor_idx]\n",
    "X_test = X_test_raw[:, sensor_idx].copy()\n",
    "labels_test = y_test_raw.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val   shape:\", X_val.shape)\n",
    "print(\"Test  shape:\", X_test.shape)\n",
    "print(\"Label Test shape:\", labels_test.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Fit AR(p) sur le train\n",
    "# -----------------------------\n",
    "ar_model = AutoReg(X_train, lags=p, old_names=False).fit()\n",
    "print(ar_model.summary())\n",
    "\n",
    "# Coefficients : x_t = const + sum_{k=1}^p phi_k * x_{t-k}\n",
    "const = ar_model.params[0]\n",
    "phi   = ar_model.params[1:]   # shape (p,)\n",
    "\n",
    "print(\"AR order       :\", len(phi))\n",
    "print(\"Const          :\", const)\n",
    "\n",
    "# Prédiction sur le jeu de validation\n",
    "history_val = list(X_val[:p])\n",
    "val_pred = []\n",
    "for t in range(X_val.shape[0]-p):\n",
    "    past = np.array(history_val[-p:])   # shape (p,)\n",
    "    lags = past[::-1]    # [x_{t-1}, ..., x_{t-p}]\n",
    "    y_hat = const + np.dot(phi, lags)\n",
    "\n",
    "    val_pred.append(y_hat)\n",
    "    history_val.append(X_val[p+t])\n",
    "    \n",
    "val_pred = np.array(val_pred)\n",
    "val_true = X_val[p:]\n",
    "\n",
    "# Prédiction sur le jeu de test\n",
    "history = list(X_test[:p])\n",
    "test_pred = []\n",
    "for t in range(T_test-p):\n",
    "    past = np.array(history[-p:])   # shape (p,)\n",
    "    lags = past[::-1]    # [x_{t-1}, ..., x_{t-p}]\n",
    "    y_hat = const + np.dot(phi, lags)\n",
    "\n",
    "    test_pred.append(y_hat)\n",
    "    history.append(X_test[p+t])\n",
    "\n",
    "test_pred = np.array(test_pred)\n",
    "test_true = X_test[p:]\n",
    "labels_true = labels_test[p:]\n",
    "\n",
    "print(\"\\nForecast shapes:\")\n",
    "print(\"val_pred:\", val_pred.shape)\n",
    "print(\"val_true:\", val_true.shape)\n",
    "print(\"test_pred:\", test_pred.shape)\n",
    "print(\"test_true:\", test_true.shape)\n",
    "\n",
    "# --------------------------------------\n",
    "# Compute errors and anomaly scores\n",
    "# --------------------------------------\n",
    "# Absolute error per sensor ( PAS DE NORMALISATION ??)\n",
    "val_scores, val_err, val_a = compute_normalized_scores(val_pred, val_true, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(test_pred, test_true, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ddb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbacca6",
   "metadata": {},
   "source": [
    "## Experiment #1 : Basic replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "config = {\n",
    "    'window_size':5,\n",
    "    'prediction_horizon':1,\n",
    "    'val_ratio':0.2,\n",
    "    'seed': 42,\n",
    "    'batch_size':64,\n",
    "    'embedding_dim':64,\n",
    "    'decay': 0,\n",
    "    'epoch': 50,\n",
    "    'early_stop_win': 10,\n",
    "    'beta': (0.9, 0.999)\n",
    "}\n",
    "config['save_path'] = \"./model_normal/gdn_smap_w_\"+str(config['window_size'])+\"_epoch\"+str(config['epoch'])+\".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b51e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train, X_val = np.split(X_train_raw, [int((1 - config['val_ratio']) * len(X_train_raw))])\n",
    "\n",
    "# --- Sliding Window Creation ---\n",
    "X_train_windows = create_sliding_windows(X_train, config['window_size'])\n",
    "y_train_windows = X_train[config['window_size']:, :]\n",
    "X_val_windows = create_sliding_windows(X_val, config['window_size'])\n",
    "y_val_windows = X_val[config['window_size']:, :]\n",
    "X_test_windows = create_sliding_windows(X_test_raw, config['window_size'])\n",
    "y_test_windows = X_test_raw[config['window_size']:, :]\n",
    "labels_test_windows = y_test_raw[config['window_size']:]\n",
    "\n",
    "# Shape inspection\n",
    "print(\"--- Sliding Window Creation ---\")\n",
    "print(f\"X_train_windows shape: {X_train_windows.shape}\") \n",
    "print(f\"y_train_windows shape: {y_train_windows.shape}\")\n",
    "print(f\"X_val_windows shape: {X_val_windows.shape}\")\n",
    "print(f\"y_val_windows shape: {y_val_windows.shape}\")\n",
    "print(f\"X_test_windows shape: {X_test_windows.shape}\")\n",
    "print(f\"y_test_windows shape: {y_test_windows.shape}\")\n",
    "print(f\"labels_test_windows shape: {labels_test_windows.shape}\")\n",
    "\n",
    "# --- DataLoader Creation ---\n",
    "X_train_tensor = torch.from_numpy(X_train_windows)\n",
    "y_train_tensor = torch.from_numpy(y_train_windows)\n",
    "X_val_tensor = torch.from_numpy(X_val_windows)\n",
    "y_val_tensor = torch.from_numpy(y_val_windows)\n",
    "X_test_tensor = torch.from_numpy(X_test_windows)\n",
    "y_test_tensor = torch.from_numpy(y_test_windows)\n",
    "label_test_tensor = torch.from_numpy(labels_test_windows)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, label_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"\\n--- DataLoader Creation Complete ---\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "print(\"Data is ready for the GDN model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353e39c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Create a fully connected undirected graph among features\n",
    "nodes = torch.arange(D_features, device=device)\n",
    "pairs = torch.combinations(nodes, r=2)          # shape (E, 2)\n",
    "row = torch.cat([pairs[:, 0], pairs[:, 1]])     # source\n",
    "col = torch.cat([pairs[:, 1], pairs[:, 0]])     # cible\n",
    "edge_index = torch.stack([row, col], dim=0)     # shape (2, E)\n",
    "edge_index_sets = [edge_index]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "# Should print: (2, number_of_edges)\n",
    "\n",
    "# Instantiate GDN with the paper's hyperparameters\n",
    "model = GDN(\n",
    "    edge_index_sets=edge_index_sets,\n",
    "    node_num=D_features,\n",
    "    dim=config['embedding_dim'],                 # embedding dimension | original GDN: 64 or 128\n",
    "    out_layer_inter_dim=config.get('out_layer_inter_dim', 64), # | original GDN: 64 or 128\n",
    "    input_dim=config['window_size'],    # temporal window size | original GDN: 5\n",
    "    out_layer_num=config.get('out_layer_num', 1),  # number of MLP layers in output layer | original GDN: 1\n",
    "    topk=config.get('topk', 15)             # number of neighbors to keep in learned graph | original GDN: 15 or 30\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train(\n",
    "    model=model,\n",
    "    save_path=config['save_path'],\n",
    "    config=config,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader\n",
    ")\n",
    "print(\"Training finished. Best model saved to:\", config['save_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7cb3f",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c534a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load the best model later, uncomment the following line:\n",
    "# model.load_state_dict(torch.load(config['save_path'], map_location=device))\n",
    "\n",
    "# --- Evaluation on Validation and Test Sets ---\n",
    "val_loss, val_result = test(model, val_loader)\n",
    "test_loss, test_result = test(model, test_loader)\n",
    "print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "pred_val, gt_val = val_result\n",
    "pred_test, gt_test, labels_test = test_result\n",
    "\n",
    "pred_val = np.array(pred_val)\n",
    "gt_val = np.array(gt_val)\n",
    "pred_test = np.array(pred_test)\n",
    "gt_test = np.array(gt_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# Compute normalized anomaly scores\n",
    "val_scores, val_err, val_a = compute_normalized_scores(pred_val, gt_val, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(pred_test, gt_test, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaccc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab42a0",
   "metadata": {},
   "source": [
    "## Experiment #2 : Adapting model outputs and train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae997b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_loss(y_pred, y_true, lambda_cont=1.0, lambda_bin=24):\n",
    "    # y_pred, y_true : (batch_size, 25)\n",
    "    y_cont_pred = y_pred[:, 0]      # continuous sensor\n",
    "    y_cont_true = y_true[:, 0]\n",
    "\n",
    "    y_bin_pred = y_pred[:, 1:]      # logits for binary sensors\n",
    "    y_bin_true = y_true[:, 1:].float()\n",
    "\n",
    "    # 1) continuous: MSE\n",
    "    loss_cont = F.mse_loss(y_cont_pred, y_cont_true)\n",
    "\n",
    "    # 2) binary: BCEWithLogits\n",
    "    loss_bin = F.binary_cross_entropy_with_logits(y_bin_pred, y_bin_true)\n",
    "\n",
    "    return lambda_cont * loss_cont + lambda_bin * loss_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "config = {\n",
    "    'window_size':100,\n",
    "    'prediction_horizon':1,\n",
    "    'val_ratio':0.2,\n",
    "    'seed': 42,\n",
    "    'batch_size':64,\n",
    "    'embedding_dim':64,\n",
    "    'decay': 0,\n",
    "    'epoch': 50,\n",
    "    'early_stop_win': 10,\n",
    "    'beta': (0.9, 0.999)\n",
    "}\n",
    "config['save_path'] = \"./model_mixed/gdn_smap_w_\"+str(config['window_size'])+\"_epoch\"+str(config['epoch'])+\".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b641e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train, X_val = np.split(X_train_raw, [int((1 - config['val_ratio']) * len(X_train_raw))])\n",
    "\n",
    "# --- Sliding Window Creation ---\n",
    "X_train_windows = create_sliding_windows(X_train, config['window_size'])\n",
    "y_train_windows = X_train[config['window_size']:, :]\n",
    "X_val_windows = create_sliding_windows(X_val, config['window_size'])\n",
    "y_val_windows = X_val[config['window_size']:, :]\n",
    "X_test_windows = create_sliding_windows(X_test_raw, config['window_size'])\n",
    "y_test_windows = X_test_raw[config['window_size']:, :]\n",
    "labels_test_windows = y_test_raw[config['window_size']:]\n",
    "\n",
    "# Shape inspection\n",
    "print(\"--- Sliding Window Creation ---\")\n",
    "print(f\"X_train_windows shape: {X_train_windows.shape}\") \n",
    "print(f\"y_train_windows shape: {y_train_windows.shape}\")\n",
    "print(f\"X_val_windows shape: {X_val_windows.shape}\")\n",
    "print(f\"y_val_windows shape: {y_val_windows.shape}\")\n",
    "print(f\"X_test_windows shape: {X_test_windows.shape}\")\n",
    "print(f\"y_test_windows shape: {y_test_windows.shape}\")\n",
    "print(f\"labels_test_windows shape: {labels_test_windows.shape}\")\n",
    "\n",
    "# --- DataLoader Creation ---\n",
    "X_train_tensor = torch.from_numpy(X_train_windows)\n",
    "y_train_tensor = torch.from_numpy(y_train_windows)\n",
    "X_val_tensor = torch.from_numpy(X_val_windows)\n",
    "y_val_tensor = torch.from_numpy(y_val_windows)\n",
    "X_test_tensor = torch.from_numpy(X_test_windows)\n",
    "y_test_tensor = torch.from_numpy(y_test_windows)\n",
    "label_test_tensor = torch.from_numpy(labels_test_windows)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, label_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"\\n--- DataLoader Creation Complete ---\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "print(\"Data is ready for the GDN model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a603d8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Create a fully connected undirected graph among features\n",
    "nodes = torch.arange(D_features, device=device)\n",
    "pairs = torch.combinations(nodes, r=2)          # shape (E, 2)\n",
    "row = torch.cat([pairs[:, 0], pairs[:, 1]])     # source\n",
    "col = torch.cat([pairs[:, 1], pairs[:, 0]])     # cible\n",
    "edge_index = torch.stack([row, col], dim=0)     # shape (2, E)\n",
    "edge_index_sets = [edge_index]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "# Should print: (2, number_of_edges)\n",
    "\n",
    "# Instantiate GDN with the paper's hyperparameters\n",
    "model = GDN(\n",
    "    edge_index_sets=edge_index_sets,\n",
    "    node_num=D_features,\n",
    "    dim=config['embedding_dim'],                 # embedding dimension | original GDN: 64 or 128\n",
    "    out_layer_inter_dim=config.get('out_layer_inter_dim', 64), # | original GDN: 64 or 128\n",
    "    input_dim=config['window_size'],    # temporal window size | original GDN: 5\n",
    "    out_layer_num=config.get('out_layer_num', 1),  # number of MLP layers in output layer | original GDN: 1\n",
    "    topk=config.get('topk', 15)             # number of neighbors to keep in learned graph | original GDN: 15 or 30\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train(\n",
    "    model=model,\n",
    "    save_path=config['save_path'],\n",
    "    config=config,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    loss_func=partial(mixed_loss, lambda_cont=1.0, lambda_bin=24)\n",
    ")\n",
    "print(\"Training finished. Best model saved to:\", config['save_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2586e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67df14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load the best model later, uncomment the following line:\n",
    "# model.load_state_dict(torch.load(config['save_path'], map_location=device))\n",
    "\n",
    "# --- Evaluation on Validation and Test Sets ---\n",
    "val_loss, val_result = test(model, val_loader, loss_func=partial(mixed_loss, lambda_cont=1.0, lambda_bin=24))\n",
    "test_loss, test_result = test(model, test_loader, loss_func=partial(mixed_loss, lambda_cont=1.0, lambda_bin=24) )\n",
    "print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "pred_val, gt_val = val_result\n",
    "pred_test, gt_test, labels_test = test_result\n",
    "\n",
    "pred_val = np.array(pred_val)\n",
    "gt_val = np.array(gt_val)\n",
    "pred_test = np.array(pred_test)\n",
    "gt_test = np.array(gt_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# Compute normalized anomaly scores\n",
    "val_scores, val_err, val_a = compute_normalized_scores(pred_val, gt_val, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(pred_test, gt_test, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold selection based on validation set\n",
    "\n",
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8180682",
   "metadata": {},
   "source": [
    "## Experiments #3 : Anomaly in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'window_size': 5,\n",
    "    'prediction_horizon': 1,\n",
    "    'val_ratio': 0.2,\n",
    "    'epoch': 50,\n",
    "    'early_stop_win': 10,\n",
    "    'batch_size': 32,\n",
    "    'decay': 0,\n",
    "    'beta': (0.9, 0.999),\n",
    "    'emb_dim': 64,                 # embedding dimension | original GDN: 64 or 128\n",
    "    'out_layer_inter_dim': 64,  # | original GDN: 64 or 128\n",
    "    'out_layer_num': 1,          # number of layers in output MLP | original GDN: 1 or 2\n",
    "    'topk': 15           # number of neighbors to keep in learned graph\n",
    "}\n",
    "config['save_path'] = \"./model_normal/gdn_smap\"+str(config['epoch'])+\".pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436bd8f",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train, X_test = np.split(X_test_raw, [int(0.33 * len(X_test_raw))])\n",
    "lambda_test = y_test_raw[int(0.33 * len(y_test_raw)):]\n",
    "X_train, X_val = np.split(X_train, [int((1 - config['val_ratio']) * len(X_train))])\n",
    "\n",
    "# --- Sliding Window Creation ---\n",
    "X_train_windows = create_sliding_windows(X_train, config['window_size'])\n",
    "y_train_windows = X_train[config['window_size']:, :]\n",
    "X_val_windows = create_sliding_windows(X_val, config['window_size'])\n",
    "y_val_windows = X_val[config['window_size']:, :]\n",
    "X_test_windows = create_sliding_windows(X_test, config['window_size'])\n",
    "y_test_windows = X_test[config['window_size']:, :]\n",
    "labels_test_windows = lambda_test[config['window_size']:]\n",
    "\n",
    "# Shape inspection\n",
    "print(\"--- Sliding Window Creation ---\")\n",
    "print(f\"X_train_windows shape: {X_train_windows.shape}\") \n",
    "print(f\"y_train_windows shape: {y_train_windows.shape}\")\n",
    "print(f\"X_val_windows shape: {X_val_windows.shape}\")\n",
    "print(f\"y_val_windows shape: {y_val_windows.shape}\")\n",
    "print(f\"X_test_windows shape: {X_test_windows.shape}\")\n",
    "print(f\"y_test_windows shape: {y_test_windows.shape}\")\n",
    "print(f\"labels_test_windows shape: {labels_test_windows.shape}\")\n",
    "\n",
    "# --- DataLoader Creation ---\n",
    "X_train_tensor = torch.from_numpy(X_train_windows)\n",
    "y_train_tensor = torch.from_numpy(y_train_windows)\n",
    "X_val_tensor = torch.from_numpy(X_val_windows)\n",
    "y_val_tensor = torch.from_numpy(y_val_windows)\n",
    "X_test_tensor = torch.from_numpy(X_test_windows)\n",
    "y_test_tensor = torch.from_numpy(y_test_windows)\n",
    "label_test_tensor = torch.from_numpy(labels_test_windows)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, label_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"\\n--- DataLoader Creation Complete ---\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "print(\"Data is ready for the GDN model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa7922",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bec8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Create a fully connected undirected graph among features\n",
    "nodes = torch.arange(D_features, device=device)\n",
    "pairs = torch.combinations(nodes, r=2)          # shape (E, 2)\n",
    "row = torch.cat([pairs[:, 0], pairs[:, 1]])     # source\n",
    "col = torch.cat([pairs[:, 1], pairs[:, 0]])     # cible\n",
    "edge_index = torch.stack([row, col], dim=0)     # shape (2, E)\n",
    "edge_index_sets = [edge_index]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "# Should print: (2, number_of_edges)\n",
    "\n",
    "# Instantiate GDN with the paper's hyperparameters\n",
    "model = GDN(\n",
    "    edge_index_sets=edge_index_sets,\n",
    "    node_num=D_features,\n",
    "    dim=config['emb_dim'],                 # embedding dimension | original GDN: 64 or 128\n",
    "    out_layer_inter_dim=config['out_layer_inter_dim'], # | original GDN: 64 or 128\n",
    "    input_dim=config['window_size'],    # temporal window size | original GDN: 5\n",
    "    out_layer_num=config['out_layer_num'],\n",
    "    topk=config['topk']               # number of neighbors to keep in learned graph | original GDN: 15 or 30\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05febbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train(\n",
    "    model=model,\n",
    "    save_path=config['save_path'],\n",
    "    config=config,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader\n",
    ")\n",
    "\n",
    "print(\"Training finished. Best model saved to:\", config['save_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a652b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb97351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load the best model later, uncomment the following line:\n",
    "# model.load_state_dict(torch.load(config['save_path'], map_location=device))\n",
    "\n",
    "# --- Evaluation on Validation and Test Sets ---\n",
    "val_loss, val_result = test(model, val_loader)\n",
    "test_loss, test_result = test(model, test_loader)\n",
    "print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "pred_val, gt_val = val_result\n",
    "pred_test, gt_test, labels_test = test_result\n",
    "\n",
    "pred_val = np.array(pred_val)\n",
    "gt_val = np.array(gt_val)\n",
    "pred_test = np.array(pred_test)\n",
    "gt_test = np.array(gt_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# Compute normalized anomaly scores\n",
    "val_scores, val_err, val_a = compute_normalized_scores(pred_val, gt_val, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(pred_test, gt_test, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold selection based on validation set\n",
    "\n",
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a52b5",
   "metadata": {},
   "source": [
    "## Expermient #4 : Changing window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'prediction_horizon': 1,\n",
    "    'val_ratio': 0.2,\n",
    "    'epoch': 50,\n",
    "    'early_stop_win': 10,\n",
    "    'batch_size': 32,\n",
    "    'decay': 0,\n",
    "    'beta': (0.9, 0.999),\n",
    "    'emb_dim': 64,                 # embedding dimension | original GDN: 64 or 128\n",
    "    'out_layer_inter_dim': 64,  # | original GDN: 64 or 128\n",
    "    'out_layer_num': 1,          # number of layers in output MLP | original GDN: 1 or 2\n",
    "    'topk': 15           # number of neighbors to keep in learned graph\n",
    "}\n",
    "\n",
    "window_list = [5, 10, 20, 50, 75, 100, 150, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed0d48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a667e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val = np.split(X_train_raw, [int((1 - config['val_ratio']) * len(X_train_raw))])\n",
    "\n",
    "# Create a fully connected undirected graph among features\n",
    "nodes = torch.arange(D_features, device=device)\n",
    "pairs = torch.combinations(nodes, r=2)          # shape (E, 2)\n",
    "row = torch.cat([pairs[:, 0], pairs[:, 1]])     # source\n",
    "col = torch.cat([pairs[:, 1], pairs[:, 0]])     # cible\n",
    "edge_index = torch.stack([row, col], dim=0)     # shape (2, E)\n",
    "edge_index_sets = [edge_index]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "# Should print: (2, number_of_edges)\n",
    "\n",
    "AUC_per_W = {}\n",
    "\n",
    "for W in window_list:\n",
    "    config['window_size'] = W\n",
    "    config['save_path'] = \"./model_normal/gdn_smap_w\"+str(W)+\"_epochs_\"+str(config['epoch'])+\".pth\"\n",
    "    \n",
    "    # --- Sliding Window Creation ---\n",
    "    X_train_windows = create_sliding_windows(X_train, config['window_size'])\n",
    "    y_train_windows = X_train[config['window_size']:, :]\n",
    "    X_val_windows = create_sliding_windows(X_val, config['window_size'])\n",
    "    y_val_windows = X_val[config['window_size']:, :]\n",
    "    X_test_windows = create_sliding_windows(X_test_raw, config['window_size'])\n",
    "    y_test_windows = X_test_raw[config['window_size']:, :]\n",
    "    labels_test_windows = y_test_raw[config['window_size']:]\n",
    "\n",
    "    X_train_tensor = torch.from_numpy(X_train_windows)\n",
    "    y_train_tensor = torch.from_numpy(y_train_windows)\n",
    "    X_val_tensor = torch.from_numpy(X_val_windows)\n",
    "    y_val_tensor = torch.from_numpy(y_val_windows)\n",
    "    X_test_tensor = torch.from_numpy(X_test_windows)\n",
    "    y_test_tensor = torch.from_numpy(y_test_windows)\n",
    "    label_test_tensor = torch.from_numpy(labels_test_windows)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor, label_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    model = GDN(\n",
    "        edge_index_sets=edge_index_sets,\n",
    "        node_num=D_features,\n",
    "        dim=config['embedding_dim'],                 # embedding dimension | original GDN: 64 or 128\n",
    "        out_layer_inter_dim=config.get('out_layer_inter_dim', 64), # | original GDN: 64 or 128\n",
    "        input_dim=config['window_size'],    # temporal window size | original GDN: 5\n",
    "        out_layer_num=config.get('out_layer_num', 1),  # number of MLP layers in output layer | original GDN: 1\n",
    "        topk=config.get('topk', 15)             # number of neighbors to keep in learned graph | original GDN: 15 or 30\n",
    "    ).to(device)\n",
    "    train_losses = train(\n",
    "        model=model,\n",
    "        save_path=config['save_path'],\n",
    "        config=config,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader\n",
    "    )\n",
    "    print(\"Training finished. Best model saved to:\", config['save_path'])\n",
    "    \n",
    "    # --- Evaluation on Validation and Test Sets ---\n",
    "    val_loss, val_result = test(model, val_loader)\n",
    "    test_loss, test_result = test(model, test_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    pred_val, gt_val = val_result\n",
    "    pred_test, gt_test, labels_test = test_result\n",
    "\n",
    "    pred_val = np.array(pred_val)\n",
    "    gt_val = np.array(gt_val)\n",
    "    pred_test = np.array(pred_test)\n",
    "    gt_test = np.array(gt_test)\n",
    "    labels_test = np.array(labels_test)\n",
    "\n",
    "    # Compute normalized anomaly scores\n",
    "    val_scores, val_err, val_a = compute_normalized_scores(pred_val, gt_val, smooth_window=3)\n",
    "    test_scores, test_err, test_a = compute_normalized_scores(pred_test, gt_test, smooth_window=3)\n",
    "    threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "    # Strict point-wise evaluation\n",
    "    strict_metrics = evaluate_anomaly_detection(test_scores, labels_test, threshold)\n",
    "    AUC_per_W[W] = strict_metrics['auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8255d7",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b43bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(list(AUC_per_W.keys()), list(AUC_per_W.values()), marker='o', linewidth=2)\n",
    "plt.xlabel(\"Window Size (w)\", fontsize=12)\n",
    "plt.ylabel(\"point-wise AUC\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.xticks(list(AUC_per_W.keys()))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f81cf0",
   "metadata": {},
   "source": [
    "# SMD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4dbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Configuration and File Paths (SMD)\n",
    "# -----------------------------\n",
    "BASE_DIR = './data'\n",
    "TRAIN_DATA_PATH = BASE_DIR + '/SMD/SMD/SMD_train.npy'\n",
    "TEST_DATA_PATH  = BASE_DIR + '/SMD/SMD/SMD_test.npy'\n",
    "TEST_LABEL_PATH = BASE_DIR + '/SMD/SMD/SMD_test_label.npy'\n",
    "\n",
    "print(\"--- Loading SMD Raw Data ---\")\n",
    "try:\n",
    "    X_train_raw = np.load(TRAIN_DATA_PATH)\n",
    "    X_test_raw  = np.load(TEST_DATA_PATH)\n",
    "    y_test_raw  = np.load(TEST_LABEL_PATH)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Downloading SMD dataset via torch_timeseries...\")\n",
    "    from torch_timeseries.dataset import SMD\n",
    "    SMD(root=BASE_DIR)\n",
    "    print(\"SMD dataset downloaded.\")\n",
    "    X_train_raw = np.load(TRAIN_DATA_PATH)\n",
    "    X_test_raw  = np.load(TEST_DATA_PATH)\n",
    "    y_test_raw  = np.load(TEST_LABEL_PATH)\n",
    "\n",
    "print(\"SMD Data Loaded Successfully.\")\n",
    "T_train, D_features = X_train_raw.shape\n",
    "T_test,  _         = X_test_raw.shape\n",
    "print(f\"SMD train shape: {X_train_raw.shape}\")\n",
    "print(f\"SMD test  shape: {X_test_raw.shape}\")\n",
    "print(f\"SMD labels shape: {y_test_raw.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adde853",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a5886",
   "metadata": {},
   "source": [
    "### Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_indices = [0,4,10,15]  # Example feature indices to plot\n",
    "features_to_plot = X_train_raw[:, feature_indices]\n",
    "time_index = np.arange(T_train)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 10), sharex=True)\n",
    "axes = axes.flatten()\n",
    "for i, feature_index in enumerate(feature_indices):\n",
    "    ax = axes[i] \n",
    "    ax.plot(\n",
    "        time_index, \n",
    "        features_to_plot[:, i], \n",
    "        label=f'Feature D{feature_index}', \n",
    "        linewidth=0.5\n",
    "    )\n",
    "    # Set title, labels, and grid\n",
    "    ax.set_title(f'Feature D{feature_index} (Raw Data)', fontsize=14)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "axes[3].set_xlabel('Time Step', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423f35e",
   "metadata": {},
   "source": [
    "Plotting the anomaly occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb717b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "time_index = np.arange(T_test)\n",
    "# Iterate through the selected features and plot them\n",
    "for i, feature_index in enumerate(feature_indices):\n",
    "    plt.plot(\n",
    "        time_index, \n",
    "        y_test_raw,\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "plt.title(f'Anomalies in Test Data', fontsize=16)\n",
    "plt.xlabel('Time Step', fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"proportion of anomalies: {np.mean(y_test_raw):.4f} in test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc7b9c",
   "metadata": {},
   "source": [
    "### Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nan_inf(X_train_raw, \"X_train_raw\")\n",
    "check_nan_inf(X_test_raw, \"X_test_raw\")\n",
    "check_nan_inf(y_test_raw, \"y_test_raw\")\n",
    "\n",
    "print(\"\\n--- Basic Statistics for X_train_raw ---\")\n",
    "basic_statistics(X_train_raw)\n",
    "\n",
    "print(\"\\n--- Basic Statistics for X_test_raw ---\")\n",
    "basic_statistics(X_test_raw)\n",
    "\n",
    "print(\"\\n--- Basic Statistics for y_test_raw ---\")\n",
    "stat_labels(y_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718530e",
   "metadata": {},
   "source": [
    "### Further analisis for stationarity, noise check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfda182",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id = 16 # We choose the sensor we want to analyze\n",
    "data_analyze(X_train_raw[:, sensor_id], sensor_id=sensor_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060b46f",
   "metadata": {},
   "source": [
    "### Detrending experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id = 0   # sensor 0 \n",
    "\n",
    "# TRAIN\n",
    "x0_train = X_train_raw[:, sensor_id]\n",
    "x0_train_detrended, trend_train = detrend_least_squares(x0_train, order=2)\n",
    "\n",
    "# TEST\n",
    "x0_test = X_test_raw[:, sensor_id]\n",
    "x0_test_detrended, trend_test = detrend_least_squares(x0_test, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f22212",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.plot(trend_train, label=\"Trend (fit)\")\n",
    "plt.plot(x0_train_detrended, label=\"Detrended\")\n",
    "plt.plot(x0_train, label=\"Original\")\n",
    "plt.legend()\n",
    "plt.title(\"Sensor 0 detrending (train)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0599e52",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7851f6f",
   "metadata": {},
   "source": [
    "### VAR(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Fit a multivariate VAR(100)\n",
    "# -----------------------------\n",
    "\n",
    "val_ratio = 0.2\n",
    "X_train, X_val = X_train_raw[:int((1 - val_ratio) * T_train)], X_train_raw[int((1 - val_ratio) * T_train):]\n",
    "X_test = X_test_raw.copy()\n",
    "labels_test = y_test_raw.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val   shape:\", X_val.shape)\n",
    "print(\"Test  shape:\", X_test.shape)\n",
    "print(\"Label Test shape:\", labels_test.shape)\n",
    "\n",
    "# Fit VAR on training data\n",
    "p = 100  # VAR order\n",
    "print(f\"\\nFitting VAR({p}) on all {X_train.shape[1]} sensors...\")\n",
    "var_model = VAR(X_train)\n",
    "var_results = var_model.fit(maxlags=p, ic=None, trend='c')  # fixed lag order\n",
    "try:\n",
    "    print(var_results.summary())\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"Warning: covariance not positive definite, skipping textual summary.\")\n",
    "# Effective order used (should be p)\n",
    "print(\"VAR lag order used (k_ar):\", var_results.k_ar)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) One-step-ahead rolling forecast on test\n",
    "# -----------------------------------------\n",
    "\n",
    "T_test, _ = X_test.shape\n",
    "k_ar = var_results.k_ar  # should be 100\n",
    "print(f\"k_ar: {k_ar}\")\n",
    "\n",
    "# Prediction on val\n",
    "history_val = list(X_val[:k_ar])  # list of length k_ar of vectors (D_features,)\n",
    "val_pred = []  # will store one-step-ahead predictions for the val set\n",
    "for t in range(X_val.shape[0]-k_ar):\n",
    "    # Take last k_ar observations as a (k_ar, D) array\n",
    "    past = np.array(history_val[-k_ar:])   # shape (k_ar, D_features)\n",
    "    \n",
    "    # Forecast the next step (t+1)\n",
    "    y_hat = var_results.forecast(past, steps=1)[0]  # shape (D_features,)\n",
    "    val_pred.append(y_hat)\n",
    "    \n",
    "    # Update history with the *true* val point at time t\n",
    "    history_val.append(X_val[k_ar+t])\n",
    "val_pred = np.array(val_pred) # shape (T_val-k_ar, D_features)\n",
    "val_true = X_val[k_ar:]  # true values from time k_ar to T_val-1\n",
    "\n",
    "# Prediction on test\n",
    "history = list(X_test[:k_ar])  # list of length k_ar of vectors (D_features,)\n",
    "test_pred = []  # will store one-step-ahead predictions for the test set\n",
    "for t in range(T_test-k_ar):\n",
    "    # Take last k_ar observations as a (k_ar, D) array\n",
    "    past = np.array(history[-k_ar:])   # shape (k_ar, D_features)\n",
    "    \n",
    "    # Forecast the next step (t+1)\n",
    "    y_hat = var_results.forecast(past, steps=1)[0]  # shape (D_features,)\n",
    "    test_pred.append(y_hat)\n",
    "    \n",
    "    # Update history with the *true* test point at time t\n",
    "    history.append(X_test[k_ar+t])\n",
    "\n",
    "test_pred = np.array(test_pred) # shape (T_test-k_ar, D_features)\n",
    "test_true = X_test[k_ar:]  # true values from time k_ar to T_test-1\n",
    "labels_true = labels_test[k_ar:]  # true labels from time k_ar to T_test-1\n",
    "\n",
    "print(\"\\nForecast shapes:\")\n",
    "print(\"val_pred:\", val_pred.shape)\n",
    "print(\"val_true:\", val_true.shape)\n",
    "print(\"test_pred:\", test_pred.shape)\n",
    "print(\"test_true:\", test_true.shape)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3) Compute errors and anomaly scores\n",
    "# --------------------------------------\n",
    "\n",
    "val_scores, val_err, val_a = compute_normalized_scores(val_pred, val_true, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(test_pred, test_true, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91baa9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaed31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54151e36",
   "metadata": {},
   "source": [
    "### AR(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Paramètres\n",
    "# -----------------------------\n",
    "sensor_idx = 0           # on ne travaille que sur le sensor 0\n",
    "p = 100          # ordre du modèle AR, ex. 100\n",
    "\n",
    "val_ratio = 0.2\n",
    "X_train, X_val = X_train_raw[:int((1 - val_ratio) * T_train),sensor_idx], X_train_raw[int((1 - val_ratio) * T_train):,sensor_idx]\n",
    "X_test = X_test_raw[:, sensor_idx].copy()\n",
    "labels_test = y_test_raw.copy()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val   shape:\", X_val.shape)\n",
    "print(\"Test  shape:\", X_test.shape)\n",
    "print(\"Label Test shape:\", labels_test.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Fit AR(p) sur le train\n",
    "# -----------------------------\n",
    "ar_model = AutoReg(X_train, lags=p, old_names=False).fit()\n",
    "print(ar_model.summary())\n",
    "\n",
    "# Coefficients : x_t = const + sum_{k=1}^p phi_k * x_{t-k}\n",
    "const = ar_model.params[0]\n",
    "phi   = ar_model.params[1:]   # shape (p,)\n",
    "\n",
    "print(\"AR order       :\", len(phi))\n",
    "print(\"Const          :\", const)\n",
    "\n",
    "# Prédiction sur le jeu de validation\n",
    "history_val = list(X_val[:p])\n",
    "val_pred = []\n",
    "for t in range(X_val.shape[0]-p):\n",
    "    past = np.array(history_val[-p:])   # shape (p,)\n",
    "    lags = past[::-1]    # [x_{t-1}, ..., x_{t-p}]\n",
    "    y_hat = const + np.dot(phi, lags)\n",
    "\n",
    "    val_pred.append(y_hat)\n",
    "    history_val.append(X_val[p+t])\n",
    "    \n",
    "val_pred = np.array(val_pred)\n",
    "val_true = X_val[p:]\n",
    "\n",
    "# Prédiction sur le jeu de test\n",
    "history = list(X_test[:p])\n",
    "test_pred = []\n",
    "for t in range(T_test-p):\n",
    "    past = np.array(history[-p:])   # shape (p,)\n",
    "    lags = past[::-1]    # [x_{t-1}, ..., x_{t-p}]\n",
    "    y_hat = const + np.dot(phi, lags)\n",
    "\n",
    "    test_pred.append(y_hat)\n",
    "    history.append(X_test[p+t])\n",
    "\n",
    "test_pred = np.array(test_pred)\n",
    "test_true = X_test[p:]\n",
    "labels_true = labels_test[p:]\n",
    "\n",
    "print(\"\\nForecast shapes:\")\n",
    "print(\"val_pred:\", val_pred.shape)\n",
    "print(\"val_true:\", val_true.shape)\n",
    "print(\"test_pred:\", test_pred.shape)\n",
    "print(\"test_true:\", test_true.shape)\n",
    "\n",
    "# --------------------------------------\n",
    "# Compute errors and anomaly scores\n",
    "# --------------------------------------\n",
    "# Absolute error per sensor ( PAS DE NORMALISATION ??)\n",
    "val_scores, val_err, val_a = compute_normalized_scores(val_pred, val_true, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(test_pred, test_true, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef96fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9757c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_true, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfffae6",
   "metadata": {},
   "source": [
    "## Experiment #1 : Basic replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "config = {\n",
    "    'window_size':5,\n",
    "    'prediction_horizon':1,\n",
    "    'val_ratio':0.2,\n",
    "    'seed': 42,\n",
    "    'batch_size':64,\n",
    "    'embedding_dim':64,\n",
    "    'decay': 0,\n",
    "    'epoch': 50,\n",
    "    'early_stop_win': 10,\n",
    "    'beta': (0.9, 0.999)\n",
    "}\n",
    "config['save_path'] = \"./model_normal/gdn_smap_w_\"+str(config['window_size'])+\"_epoch\"+str(config['epoch'])+\".pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fee142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train, X_val = np.split(X_train_raw, [int((1 - config['val_ratio']) * len(X_train_raw))])\n",
    "\n",
    "# --- Sliding Window Creation ---\n",
    "X_train_windows = create_sliding_windows(X_train, config['window_size'])\n",
    "y_train_windows = X_train[config['window_size']:, :]\n",
    "X_val_windows = create_sliding_windows(X_val, config['window_size'])\n",
    "y_val_windows = X_val[config['window_size']:, :]\n",
    "X_test_windows = create_sliding_windows(X_test_raw, config['window_size'])\n",
    "y_test_windows = X_test_raw[config['window_size']:, :]\n",
    "labels_test_windows = y_test_raw[config['window_size']:]\n",
    "\n",
    "# Shape inspection\n",
    "print(\"--- Sliding Window Creation ---\")\n",
    "print(f\"X_train_windows shape: {X_train_windows.shape}\") \n",
    "print(f\"y_train_windows shape: {y_train_windows.shape}\")\n",
    "print(f\"X_val_windows shape: {X_val_windows.shape}\")\n",
    "print(f\"y_val_windows shape: {y_val_windows.shape}\")\n",
    "print(f\"X_test_windows shape: {X_test_windows.shape}\")\n",
    "print(f\"y_test_windows shape: {y_test_windows.shape}\")\n",
    "print(f\"labels_test_windows shape: {labels_test_windows.shape}\")\n",
    "\n",
    "# --- DataLoader Creation ---\n",
    "X_train_tensor = torch.from_numpy(X_train_windows)\n",
    "y_train_tensor = torch.from_numpy(y_train_windows)\n",
    "X_val_tensor = torch.from_numpy(X_val_windows)\n",
    "y_val_tensor = torch.from_numpy(y_val_windows)\n",
    "X_test_tensor = torch.from_numpy(X_test_windows)\n",
    "y_test_tensor = torch.from_numpy(y_test_windows)\n",
    "label_test_tensor = torch.from_numpy(labels_test_windows)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, label_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"\\n--- DataLoader Creation Complete ---\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "print(\"Data is ready for the GDN model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96dadc",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Create a fully connected undirected graph among features\n",
    "nodes = torch.arange(D_features, device=device)\n",
    "pairs = torch.combinations(nodes, r=2)          # shape (E, 2)\n",
    "row = torch.cat([pairs[:, 0], pairs[:, 1]])     # source\n",
    "col = torch.cat([pairs[:, 1], pairs[:, 0]])     # cible\n",
    "edge_index = torch.stack([row, col], dim=0)     # shape (2, E)\n",
    "edge_index_sets = [edge_index]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "# Should print: (2, number_of_edges)\n",
    "\n",
    "# Instantiate GDN with the paper's hyperparameters\n",
    "model = GDN(\n",
    "    edge_index_sets=edge_index_sets,\n",
    "    node_num=D_features,\n",
    "    dim=config['embedding_dim'],                 # embedding dimension | original GDN: 64 or 128\n",
    "    out_layer_inter_dim=config.get('out_layer_inter_dim', 64), # | original GDN: 64 or 128\n",
    "    input_dim=config['window_size'],    # temporal window size | original GDN: 5\n",
    "    out_layer_num=config.get('out_layer_num', 1),  # number of MLP layers in output layer | original GDN: 1\n",
    "    topk=config.get('topk', 15)             # number of neighbors to keep in learned graph | original GDN: 15 or 30\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train(\n",
    "    model=model,\n",
    "    save_path=config['save_path'],\n",
    "    config=config,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader\n",
    ")\n",
    "print(\"Training finished. Best model saved to:\", config['save_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c2b9e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load the best model later, uncomment the following line:\n",
    "# model.load_state_dict(torch.load(config['save_path'], map_location=device))\n",
    "\n",
    "# --- Evaluation on Validation and Test Sets ---\n",
    "val_loss, val_result = test(model, val_loader)\n",
    "test_loss, test_result = test(model, test_loader)\n",
    "print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "pred_val, gt_val = val_result\n",
    "pred_test, gt_test, labels_test = test_result\n",
    "\n",
    "pred_val = np.array(pred_val)\n",
    "gt_val = np.array(gt_val)\n",
    "pred_test = np.array(pred_test)\n",
    "gt_test = np.array(gt_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# Compute normalized anomaly scores\n",
    "val_scores, val_err, val_a = compute_normalized_scores(pred_val, gt_val, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(pred_test, gt_test, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52433704",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328e1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffb8c3",
   "metadata": {},
   "source": [
    "## Experiments #2 : Anomaly in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afafabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'window_size': 5,\n",
    "    'prediction_horizon': 1,\n",
    "    'val_ratio': 0.2,\n",
    "    'epoch': 50,\n",
    "    'early_stop_win': 10,\n",
    "    'batch_size': 32,\n",
    "    'decay': 0,\n",
    "    'beta': (0.9, 0.999),\n",
    "    'emb_dim': 64,                 # embedding dimension | original GDN: 64 or 128\n",
    "    'out_layer_inter_dim': 64,  # | original GDN: 64 or 128\n",
    "    'out_layer_num': 1,          # number of layers in output MLP | original GDN: 1 or 2\n",
    "    'topk': 15           # number of neighbors to keep in learned graph\n",
    "}\n",
    "config['save_path'] = \"./model_normal/gdn_smap\"+str(config['epoch'])+\".pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf9c04",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train, X_test = np.split(X_test_raw, [int(0.33 * len(X_test_raw))])\n",
    "lambda_test = y_test_raw[int(0.33 * len(y_test_raw)):]\n",
    "X_train, X_val = np.split(X_train, [int((1 - config['val_ratio']) * len(X_train))])\n",
    "\n",
    "# --- Sliding Window Creation ---\n",
    "X_train_windows = create_sliding_windows(X_train, config['window_size'])\n",
    "y_train_windows = X_train[config['window_size']:, :]\n",
    "X_val_windows = create_sliding_windows(X_val, config['window_size'])\n",
    "y_val_windows = X_val[config['window_size']:, :]\n",
    "X_test_windows = create_sliding_windows(X_test, config['window_size'])\n",
    "y_test_windows = X_test[config['window_size']:, :]\n",
    "labels_test_windows = lambda_test[config['window_size']:]\n",
    "\n",
    "# Shape inspection\n",
    "print(\"--- Sliding Window Creation ---\")\n",
    "print(f\"X_train_windows shape: {X_train_windows.shape}\") \n",
    "print(f\"y_train_windows shape: {y_train_windows.shape}\")\n",
    "print(f\"X_val_windows shape: {X_val_windows.shape}\")\n",
    "print(f\"y_val_windows shape: {y_val_windows.shape}\")\n",
    "print(f\"X_test_windows shape: {X_test_windows.shape}\")\n",
    "print(f\"y_test_windows shape: {y_test_windows.shape}\")\n",
    "print(f\"labels_test_windows shape: {labels_test_windows.shape}\")\n",
    "\n",
    "# --- DataLoader Creation ---\n",
    "X_train_tensor = torch.from_numpy(X_train_windows)\n",
    "y_train_tensor = torch.from_numpy(y_train_windows)\n",
    "X_val_tensor = torch.from_numpy(X_val_windows)\n",
    "y_val_tensor = torch.from_numpy(y_val_windows)\n",
    "X_test_tensor = torch.from_numpy(X_test_windows)\n",
    "y_test_tensor = torch.from_numpy(y_test_windows)\n",
    "label_test_tensor = torch.from_numpy(labels_test_windows)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, label_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"\\n--- DataLoader Creation Complete ---\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "print(\"Data is ready for the GDN model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6813b16",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758844bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Create a fully connected undirected graph among features\n",
    "nodes = torch.arange(D_features, device=device)\n",
    "pairs = torch.combinations(nodes, r=2)          # shape (E, 2)\n",
    "row = torch.cat([pairs[:, 0], pairs[:, 1]])     # source\n",
    "col = torch.cat([pairs[:, 1], pairs[:, 0]])     # cible\n",
    "edge_index = torch.stack([row, col], dim=0)     # shape (2, E)\n",
    "edge_index_sets = [edge_index]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "# Should print: (2, number_of_edges)\n",
    "\n",
    "# Instantiate GDN with the paper's hyperparameters\n",
    "model = GDN(\n",
    "    edge_index_sets=edge_index_sets,\n",
    "    node_num=D_features,\n",
    "    dim=config['emb_dim'],                 # embedding dimension | original GDN: 64 or 128\n",
    "    out_layer_inter_dim=config['out_layer_inter_dim'], # | original GDN: 64 or 128\n",
    "    input_dim=config['window_size'],    # temporal window size | original GDN: 5\n",
    "    out_layer_num=config['out_layer_num'],\n",
    "    topk=config['topk']               # number of neighbors to keep in learned graph | original GDN: 15 or 30\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train(\n",
    "    model=model,\n",
    "    save_path=config['save_path'],\n",
    "    config=config,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader\n",
    ")\n",
    "\n",
    "print(\"Training finished. Best model saved to:\", config['save_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc2bb8",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1faad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to load the best model later, uncomment the following line:\n",
    "# model.load_state_dict(torch.load(config['save_path'], map_location=device))\n",
    "\n",
    "# --- Evaluation on Validation and Test Sets ---\n",
    "val_loss, val_result = test(model, val_loader)\n",
    "test_loss, test_result = test(model, test_loader)\n",
    "print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "pred_val, gt_val = val_result\n",
    "pred_test, gt_test, labels_test = test_result\n",
    "\n",
    "pred_val = np.array(pred_val)\n",
    "gt_val = np.array(gt_val)\n",
    "pred_test = np.array(pred_test)\n",
    "gt_test = np.array(gt_test)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# Compute normalized anomaly scores\n",
    "val_scores, val_err, val_a = compute_normalized_scores(pred_val, gt_val, smooth_window=3)\n",
    "test_scores, test_err, test_a = compute_normalized_scores(pred_test, gt_test, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold selection based on validation set\n",
    "\n",
    "threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "# Strict point-wise evaluation\n",
    "strict_metrics = evaluate_anomaly_detection(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {strict_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {strict_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {strict_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {strict_metrics['auc']:.4f}\")\n",
    "\n",
    "# Soft sequence-wise evaluation\n",
    "soft_metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, threshold)\n",
    "print(\"\\n=== Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {soft_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {soft_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {soft_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {soft_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best threshold based on F1-score\n",
    "\n",
    "best_f1 = 0.0\n",
    "best_threshold = None\n",
    "best_metrics = None\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Point-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")\n",
    "\n",
    "for th in np.linspace(np.min(test_scores), np.max(test_scores), num=50):\n",
    "    metrics = evaluate_anomaly_detection_on_sequences(test_scores, labels_test, th)\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        best_f1 = metrics['f1']\n",
    "        best_threshold = th\n",
    "        best_metrics = metrics\n",
    "\n",
    "print(\"\\n=== Best F1 : Sequence-wise Evaluation ===\")\n",
    "print(f\"Precision : {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {best_metrics['f1']:.4f}\")\n",
    "print(f\"AUC       : {best_metrics['auc']:.4f}\")\n",
    "print(f\"Threshold : {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f66e3",
   "metadata": {},
   "source": [
    "## Expermient #3 : Changing window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'prediction_horizon': 1,\n",
    "    'val_ratio': 0.2,\n",
    "    'epoch': 50,\n",
    "    'early_stop_win': 10,\n",
    "    'batch_size': 32,\n",
    "    'decay': 0,\n",
    "    'beta': (0.9, 0.999),\n",
    "    'emb_dim': 64,                 # embedding dimension | original GDN: 64 or 128\n",
    "    'out_layer_inter_dim': 64,  # | original GDN: 64 or 128\n",
    "    'out_layer_num': 1,          # number of layers in output MLP | original GDN: 1 or 2\n",
    "    'topk': 15           # number of neighbors to keep in learned graph\n",
    "}\n",
    "\n",
    "window_list = [5, 10, 20, 50, 75, 100, 150, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa8f7f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val = np.split(X_train_raw, [int((1 - config['val_ratio']) * len(X_train_raw))])\n",
    "\n",
    "# Create a fully connected undirected graph among features\n",
    "nodes = torch.arange(D_features, device=device)\n",
    "pairs = torch.combinations(nodes, r=2)          # shape (E, 2)\n",
    "row = torch.cat([pairs[:, 0], pairs[:, 1]])     # source\n",
    "col = torch.cat([pairs[:, 1], pairs[:, 0]])     # cible\n",
    "edge_index = torch.stack([row, col], dim=0)     # shape (2, E)\n",
    "edge_index_sets = [edge_index]\n",
    "\n",
    "print(\"edge_index shape:\", edge_index.shape)\n",
    "# Should print: (2, number_of_edges)\n",
    "\n",
    "AUC_per_W = {}\n",
    "\n",
    "for W in window_list:\n",
    "    config['window_size'] = W\n",
    "    config['save_path'] = \"./model_normal/gdn_smap_w\"+str(W)+\"_epochs_\"+str(config['epoch'])+\".pth\"\n",
    "    \n",
    "    # --- Sliding Window Creation ---\n",
    "    X_train_windows = create_sliding_windows(X_train, config['window_size'])\n",
    "    y_train_windows = X_train[config['window_size']:, :]\n",
    "    X_val_windows = create_sliding_windows(X_val, config['window_size'])\n",
    "    y_val_windows = X_val[config['window_size']:, :]\n",
    "    X_test_windows = create_sliding_windows(X_test_raw, config['window_size'])\n",
    "    y_test_windows = X_test_raw[config['window_size']:, :]\n",
    "    labels_test_windows = y_test_raw[config['window_size']:]\n",
    "\n",
    "    X_train_tensor = torch.from_numpy(X_train_windows)\n",
    "    y_train_tensor = torch.from_numpy(y_train_windows)\n",
    "    X_val_tensor = torch.from_numpy(X_val_windows)\n",
    "    y_val_tensor = torch.from_numpy(y_val_windows)\n",
    "    X_test_tensor = torch.from_numpy(X_test_windows)\n",
    "    y_test_tensor = torch.from_numpy(y_test_windows)\n",
    "    label_test_tensor = torch.from_numpy(labels_test_windows)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor, label_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    model = GDN(\n",
    "        edge_index_sets=edge_index_sets,\n",
    "        node_num=D_features,\n",
    "        dim=config['embedding_dim'],                 # embedding dimension | original GDN: 64 or 128\n",
    "        out_layer_inter_dim=config.get('out_layer_inter_dim', 64), # | original GDN: 64 or 128\n",
    "        input_dim=config['window_size'],    # temporal window size | original GDN: 5\n",
    "        out_layer_num=config.get('out_layer_num', 1),  # number of MLP layers in output layer | original GDN: 1\n",
    "        topk=config.get('topk', 15)             # number of neighbors to keep in learned graph | original GDN: 15 or 30\n",
    "    ).to(device)\n",
    "    train_losses = train(\n",
    "        model=model,\n",
    "        save_path=config['save_path'],\n",
    "        config=config,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader\n",
    "    )\n",
    "    print(\"Training finished. Best model saved to:\", config['save_path'])\n",
    "    \n",
    "    # --- Evaluation on Validation and Test Sets ---\n",
    "    val_loss, val_result = test(model, val_loader)\n",
    "    test_loss, test_result = test(model, test_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "    print(f\"Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    pred_val, gt_val = val_result\n",
    "    pred_test, gt_test, labels_test = test_result\n",
    "\n",
    "    pred_val = np.array(pred_val)\n",
    "    gt_val = np.array(gt_val)\n",
    "    pred_test = np.array(pred_test)\n",
    "    gt_test = np.array(gt_test)\n",
    "    labels_test = np.array(labels_test)\n",
    "\n",
    "    # Compute normalized anomaly scores\n",
    "    val_scores, val_err, val_a = compute_normalized_scores(pred_val, gt_val, smooth_window=3)\n",
    "    test_scores, test_err, test_a = compute_normalized_scores(pred_test, gt_test, smooth_window=3)\n",
    "    threshold = np.max(val_scores) # threshold for anomaly detection\n",
    "\n",
    "    # Strict point-wise evaluation\n",
    "    strict_metrics = evaluate_anomaly_detection(test_scores, labels_test, threshold)\n",
    "    AUC_per_W[W] = strict_metrics['auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ffcb9",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e239eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(list(AUC_per_W.keys()), list(AUC_per_W.values()), marker='o', linewidth=2)\n",
    "plt.xlabel(\"Window Size (w)\", fontsize=12)\n",
    "plt.ylabel(\"point-wise AUC\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.xticks(list(AUC_per_W.keys()))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GDN_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
